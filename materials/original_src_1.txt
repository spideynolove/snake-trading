Directory structure:
â””â”€â”€ v4/
    â”œâ”€â”€ agent.py
    â”œâ”€â”€ config.py
    â”œâ”€â”€ debug_hierarchical.py
    â”œâ”€â”€ dwx_integration.py
    â”œâ”€â”€ forex_game.py
    â”œâ”€â”€ grid_search_config.py
    â”œâ”€â”€ grid_search_trainer.py
    â”œâ”€â”€ gym_env_adapter.py
    â”œâ”€â”€ helper.py
    â”œâ”€â”€ model.py
    â”œâ”€â”€ parallel_grid_search.py
    â”œâ”€â”€ real_data_loader.py
    â”œâ”€â”€ simple_risk_manager.py
    â”œâ”€â”€ test_config.py
    â””â”€â”€ trainer.py

================================================
File: agent.py
================================================
import torch
import random
import numpy as np
from collections import deque
from forex_game import ForexGameAI, Direction
from model import Linear_QNet, QTrainer, create_model_from_config
from config import PRESET_CONFIGS

class Agent:

    def __init__(self, config_name='testing'):
        self.n_games = 0
        self.model, self.trainer, self.config = create_model_from_config(
            config_name)
        training_config = self.config['training']
        self.gamma = training_config['gamma']
        self.epsilon_start = training_config['epsilon_start']
        self.epsilon_decay = training_config['epsilon_decay']
        self.epsilon_min = training_config['epsilon_min']
        self.memory = deque(maxlen=training_config['memory_size'])
        self.batch_size = training_config['batch_size']
        self.epsilon = 0
        self.recent_performance = deque(maxlen=50)
        self.performance_threshold = 0.0
        print(
            f'âœ“ V4 Agent initialized with {config_name.upper()} configuration')
        print(f"  Memory Size: {training_config['memory_size']:,}")
        print(f"  Batch Size: {training_config['batch_size']:,}")
        print(f'  Epsilon Start: {self.epsilon_start}')
        print(f'  Epsilon Decay: {self.epsilon_decay}')
        print(f'  Epsilon Min: {self.epsilon_min}')

    def get_state(self, game):
        return game.get_h1_state()

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def train_long_memory(self):
        if len(self.memory) > self.batch_size:
            mini_sample = random.sample(self.memory, self.batch_size)
        else:
            mini_sample = self.memory
        if len(mini_sample) == 0:
            return 0.0
        states, actions, rewards, next_states, dones = zip(*mini_sample)
        return self.trainer.train_step(states, actions, rewards,
            next_states, dones)

    def train_short_memory(self, state, action, reward, next_state, done):
        return self.trainer.train_step(state, action, reward, next_state, done)

    def get_action(self, state):
        self._update_adaptive_epsilon()
        final_move = [0, 0, 0]
        if random.random() < self.epsilon:
            move = random.randint(0, 2)
            final_move[move] = 1
        else:
            state0 = torch.tensor(state, dtype=torch.float)
            with torch.no_grad():
                prediction = self.model(state0)
                move = torch.argmax(prediction).item()
            final_move[move] = 1
        return final_move

    def _update_adaptive_epsilon(self):
        if len(self.recent_performance) >= 10:
            avg_performance = np.mean(list(self.recent_performance)[-10:])
            
            if avg_performance > self.performance_threshold:
                decay_rate = 0.995
            else:
                decay_rate = 0.9999
        else:
            decay_rate = self.epsilon_decay
        
        if self.epsilon == 0:
            self.epsilon = self.epsilon_start
        else:
            self.epsilon = max(self.epsilon_min, self.epsilon * decay_rate)
    
    def update_performance(self, performance_metric):
        self.recent_performance.append(performance_metric)
    
    def get_current_epsilon(self):
        return self.epsilon

    def save_model(self, filename=None):
        if filename is None:
            filename = f'model_game_{self.n_games}.pth'
        self.model.save(filename)

    def load_model(self, filename):
        return self.model.load(filename)

def train(config_name='testing', episodes=500):
    plot_scores = []
    plot_mean_scores = []
    plot_portfolio_values = []
    plot_losses = []
    total_score = 0
    record = 0
    best_portfolio = 10000
    agent = Agent(config_name)
    from real_data_loader import load_real_gbpusd_data
    data, d1_data = load_real_gbpusd_data()
    game = ForexGameAI(data)
    print(
        f'Starting V4 Forex DQN Training - {config_name.upper()} Configuration'
        )
    print('Philosophy: Simplicity beats complexity')
    print(f'Target Episodes: {episodes}')
    print('=' * 60)
    while agent.n_games < episodes:
        state_old = agent.get_state(game)
        final_move = agent.get_action(state_old)
        reward, done, portfolio_value = game.play_step(final_move.index(1))
        state_new = agent.get_state(game)
        loss = agent.train_short_memory(state_old, final_move, reward,
            state_new, done)
        agent.remember(state_old, final_move, reward, state_new, done)
        if done:
            game.reset()
            agent.n_games += 1
            batch_loss = agent.train_long_memory()
            total_return = (portfolio_value - 10000) / 10000 * 100
            if total_return > record:
                record = total_return
                agent.save_model('best_return_model.pth')
            if portfolio_value > best_portfolio:
                best_portfolio = portfolio_value
                agent.save_model(f'best_portfolio_${int(portfolio_value)}.pth')
            current_epsilon = agent.get_current_epsilon()
            print(
                f'Episode {agent.n_games}/{episodes} | Portfolio: ${portfolio_value:.2f} | Return: {total_return:.2f}% | Record: {record:.2f}% | Îµ: {current_epsilon:.4f}'
                )
            stats = game.get_trade_stats()
            if stats['total_trades'] > 0:
                print(
                    f"  Trades: {stats['total_trades']}, Win Rate: {stats['win_rate']:.2%}, Avg P&L: ${stats['avg_pnl']:.2f}, Loss: {batch_loss:.4f}"
                    )
            plot_scores.append(total_return)
            total_score += total_return
            mean_score = total_score / agent.n_games
            plot_mean_scores.append(mean_score)
            plot_portfolio_values.append(portfolio_value)
            plot_losses.append(batch_loss if batch_loss else 0.0)
            if agent.n_games % 10 == 0:
                recent_avg = np.mean(plot_scores[-10:])
                recent_portfolio = np.mean(plot_portfolio_values[-10:])
                recent_loss = np.mean(plot_losses[-10:])
                print(
                    f'ðŸ“Š Last 10 episodes: Avg Return: {recent_avg:.2f}%, Avg Portfolio: ${recent_portfolio:.2f}, Avg Loss: {recent_loss:.4f}'
                    )
            if agent.n_games % 50 == 0:
                agent.save_model(f'checkpoint_episode_{agent.n_games}.pth')
                print(f'âœ“ Checkpoint saved at episode {agent.n_games}')
    print(f'\nðŸŽ‰ Training completed after {episodes} episodes!')
    print(f'ðŸ“ˆ Final Results:')
    print(f'  Best Return: {record:.2f}%')
    print(f'  Best Portfolio: ${best_portfolio:.2f}')
    print(f'  Final Portfolio: ${portfolio_value:.2f}')
    print(f'  Configuration: {config_name.upper()}')
    agent.save_model('final_model.pth')
    return {'scores': plot_scores, 'mean_scores': plot_mean_scores,
        'portfolio_values': plot_portfolio_values, 'losses': plot_losses,
        'best_return': record, 'best_portfolio': best_portfolio,
        'final_portfolio': portfolio_value, 'episodes': episodes, 'config':
        config_name}

def demonstrate_configs():
    from config import print_config_summary, PRESET_CONFIGS
    print('V4 Configurable DQN Architecture Demo')
    print('=' * 50)
    print('\nAvailable Configurations:')
    for config_name in PRESET_CONFIGS.keys():
        print(f'  â€¢ {config_name.upper()}')
    print('\nConfiguration Details:')
    print_config_summary('testing')
    print('\n' + '=' * 50)
    print('To train with different configurations:')
    print("  train('development')  # Fast training, simple network")
    print("  train('testing')      # Balanced for development")
    print("  train('production')   # Deep network, careful training")
    print("  train('research')     # Advanced features enabled")
    print(
        "  train('competition')  # Ultra-deep network, maximum sophistication")

if __name__ == '__main__':
    import pandas as pd
    import argparse
    parser = argparse.ArgumentParser(description='V4 Forex DQN Training')
    parser.add_argument('--config', default='testing', choices=[
        'development', 'testing', 'production', 'research', 'competition'],
        help='DQN configuration to use')
    parser.add_argument('--episodes', type=int, default=100, help=
        'Number of episodes to train')
    parser.add_argument('--demo', action='store_true', help=
        'Show configuration demo instead of training')
    args = parser.parse_args()
    if args.demo:
        demonstrate_configs()
    else:
        print(
            f'Starting training with {args.config} configuration for {args.episodes} episodes...'
            )
        results = train(args.config, args.episodes)
        print(
            f'\nTraining completed! Check ./model/ directory for saved models.'
            )

================================================
File: config.py
================================================
import torch.nn as nn

class V4Config:
    STATE_SIZE = 13
    ACTION_SIZE = 3
    NETWORK_CONFIGS = {'simple': {'hidden_layers': [256], 'dropout_rate': 
        0.0, 'activation': 'relu', 'batch_norm': False}, 'standard': {
        'hidden_layers': [512, 256, 128], 'dropout_rate': 0.2, 'activation':
        'relu', 'batch_norm': True}, 'deep': {'hidden_layers': [512, 512, 
        256, 128], 'dropout_rate': 0.3, 'activation': 'relu', 'batch_norm':
        True}, 'advanced': {'hidden_layers': [1024, 512, 512, 256, 128],
        'dropout_rate': 0.4, 'activation': 'relu', 'batch_norm': True},
        'ultra': {'hidden_layers': [2048, 1024, 512, 512, 256, 128],
        'dropout_rate': 0.5, 'activation': 'relu', 'batch_norm': True}}
    TRAINING_CONFIGS = {'fast': {'lr': 0.01, 'gamma': 0.9, 'epsilon_start':
        80, 'epsilon_decay': 0.995, 'epsilon_min': 0.01, 'memory_size': 
        50000, 'batch_size': 512, 'target_update_freq': 100}, 'standard': {
        'lr': 0.001, 'gamma': 0.95, 'epsilon_start': 15, 'epsilon_decay': 
        0.999, 'epsilon_min': 0.05, 'memory_size': 100000, 'batch_size': 
        64, 'target_update_freq': 100}, 'careful': {'lr': 0.0001,
        'gamma': 0.99, 'epsilon_start': 100, 'epsilon_decay': 0.9999,
        'epsilon_min': 0.001, 'memory_size': 200000, 'batch_size': 2000,
        'target_update_freq': 2000}}
    ADVANCED_FEATURES = {'double_dqn': True, 'dueling_dqn': True,
        'prioritized_replay': True, 'noisy_networks': False, 'multi_step': 
        3, 'gradient_clipping': 1.0}
    RISK_CONFIG = {'use_risk_features': True, 'risk_percent_per_trade': 2.0,
        'leverage': 100, 'sl_distance_pips': 15, 'tp_rr_ratio': 1.34,
        'min_position_size': 0.1, 'max_position_size': 2.0, 'pip_value': 
        0.0001, 'commission_pips': 2.0, 'risk_based_rewards': True,
        'stop_loss_integration': True}
    GPU_CONFIG = {'use_gpu': True, 'mixed_precision': True, 'data_parallel':
        True, 'device_ids': [0, 1]}

def get_activation_function(activation_name):
    activations = {'relu': nn.ReLU(), 'leaky_relu': nn.LeakyReLU(), 'elu':
        nn.ELU(), 'gelu': nn.GELU(), 'swish': nn.SiLU(), 'tanh': nn.Tanh(),
        'sigmoid': nn.Sigmoid()}
    return activations.get(activation_name, nn.ReLU())

def create_model_config(network_type='standard', training_type='standard'):
    config = V4Config()
    model_config = {'state_size': config.STATE_SIZE, 'action_size': config.
        ACTION_SIZE, 'network': config.NETWORK_CONFIGS[network_type],
        'training': config.TRAINING_CONFIGS[training_type], 'advanced':
        config.ADVANCED_FEATURES, 'risk': config.RISK_CONFIG, 'gpu': config
        .GPU_CONFIG}
    return model_config

PRESET_CONFIGS = {'development': create_model_config('simple', 'fast'),
    'testing': create_model_config('standard', 'standard'), 'production':
    create_model_config('deep', 'careful'), 'research': create_model_config
    ('advanced', 'careful'), 'competition': create_model_config('ultra',
    'careful')}

def print_config_summary(config_name='testing'):
    config = PRESET_CONFIGS[config_name]
    print(f'V4 Configuration: {config_name.upper()}')
    print('=' * 50)
    print(f"State Size: {config['state_size']} (V4 Simple)")
    print(f"Action Size: {config['action_size']} (Hold/Buy/Sell)")
    print(f"Network Layers: {config['network']['hidden_layers']}")
    print(f"Dropout Rate: {config['network']['dropout_rate']}")
    print(f"Batch Normalization: {config['network']['batch_norm']}")
    print(f"Learning Rate: {config['training']['lr']}")
    print(f"Memory Size: {config['training']['memory_size']:,}")
    print(f"Batch Size: {config['training']['batch_size']:,}")
    print(f"Double DQN: {config['advanced']['double_dqn']}")
    print(f"Dueling DQN: {config['advanced']['dueling_dqn']}")
    print(f"Prioritized Replay: {config['advanced']['prioritized_replay']}")
    print(f"GPU Acceleration: {config['gpu']['use_gpu']}")
    print(f"Mixed Precision: {config['gpu']['mixed_precision']}")

if __name__ == '__main__':
    print('Available V4 Configurations:')
    print('=' * 60)
    for config_name in PRESET_CONFIGS.keys():
        print(f'\n{config_name.upper()}:')
        config = PRESET_CONFIGS[config_name]
        layers = config['network']['hidden_layers']
        lr = config['training']['lr']
        memory = config['training']['memory_size']
        print(f'  Network: {len(layers)} layers {layers}')
        print(f'  Training: LR={lr}, Memory={memory:,}')
    print(f'\nDefault Config Summary:')
    print_config_summary('testing')

================================================
File: debug_hierarchical.py
================================================
import pandas as pd
import numpy as np
from forex_game import HierarchicalForexGameAI

def debug_hierarchical_system():
    print('Hierarchical Multi-Timeframe System Debug')
    print('=' * 50)
    from real_data_loader import load_real_gbpusd_data
    try:
        data, _ = load_real_gbpusd_data(total_samples=500)
    except:
        print('Real data loading failed, skipping debug')
        return
    game = HierarchicalForexGameAI(data)
    print(f'Total data points: {len(data)}')
    print(f"Price range: {data['close'].min():.4f} - {data['close'].max():.4f}"
        )
    print(
        f"Price trend: {(data['close'].iloc[-1] / data['close'].iloc[0] - 1) * 100:.2f}%"
        )
    game.reset()
    print(f'\nTesting first 100 steps:')
    d1_bias_changes = []
    trade_attempts = []
    for step in range(100):
        state = game.get_h1_state()
        current_bias = game.d1_trend_bias
        if step == 0 or current_bias != prev_bias:
            d1_bias_changes.append((step, current_bias))
        prev_bias = current_bias
        action = 1 if step % 10 == 0 else 0
        old_position = game.position
        reward, done, portfolio = game.play_step(action)
        new_position = game.position
        if old_position != new_position or reward != 0:
            trade_attempts.append({'step': step, 'action': action,
                'd1_bias': current_bias, 'old_pos': old_position, 'new_pos':
                new_position, 'reward': reward, 'portfolio': portfolio})
        if done:
            break
    print(f'\nD1 Trend Bias Changes:')
    for step, bias in d1_bias_changes:
        bias_name = {(-1): 'BEARISH', (0): 'FLAT', (1): 'BULLISH'}[bias]
        print(f'  Step {step}: {bias_name} ({bias})')
    print(f'\nTrade Attempts:')
    for trade in trade_attempts[:10]:
        print(
            f"  Step {trade['step']}: Action={trade['action']}, D1_bias={trade['d1_bias']}, Pos: {trade['old_pos']} -> {trade['new_pos']}, Reward={trade['reward']}"
            )
    stats = game.get_trade_stats()
    print(f'\nFinal Stats:')
    print(f"  Total trades: {stats['total_trades']}")
    print(f"  Win rate: {stats['win_rate']:.2%}")
    print(f'  Portfolio: ${portfolio:.2f}')
    print(f'  Final D1 bias: {game.d1_trend_bias}')

if __name__ == '__main__':
    debug_hierarchical_system()

================================================
File: dwx_integration.py
================================================
import sys
import os
import numpy as np
from datetime import datetime
from typing import Dict, Optional
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'v3', 'common'))
try:
    from dwx_signal_transmitter import DWXSignalTransmitter, V3SignalGenerator
    from minimal_risk_manager import MinimalRiskManager
    from simple_data_feed import SimpleDataFeed
except ImportError as e:
    print(f'Warning: Could not import V3 components: {e}')
    print('DWX integration will not be available')
from agent import Agent
from forex_game import ForexGameAI, Direction
from model import Linear_QNet

class V4DWXIntegration:

    def __init__(self, dwx_files_dir: str='/tmp/dwx_v4', initial_balance:
        float=10000):
        try:
            self.dwx_transmitter = DWXSignalTransmitter(dwx_files_dir)
            self.signal_generator = V3SignalGenerator(self.dwx_transmitter)
            self.risk_manager = MinimalRiskManager(initial_balance)
            print('âœ“ V3 DWX components initialized successfully')
        except Exception as e:
            print(f'âœ— V3 DWX components not available: {e}')
            self.dwx_transmitter = None
            self.signal_generator = None
            self.risk_manager = None
        self.live_agent = None
        self.live_data_feed = None
        self.current_position = 0
        self.position_entry_price = 0
        self.is_live_trading = False
        self.live_trades = []
        self.signals_sent = 0
        self.last_signal_time = None

    def load_trained_model(self, model_path: str) ->bool:
        try:
            self.live_agent = Agent()
            self.live_agent.model.load_state_dict(torch.load(model_path))
            self.live_agent.epsilon = 0
            print(f'âœ“ Loaded trained model from {model_path}')
            return True
        except Exception as e:
            print(f'âœ— Failed to load model: {e}')
            return False

    def start_live_trading(self, data_file_path: str, symbol: str='EURUSD'):
        if not self.live_agent:
            print('âœ— No trained model loaded')
            return False
        if not self.dwx_transmitter:
            print('âœ— DWX components not available')
            return False
        try:
            self.live_data_feed = SimpleDataFeed(data_file_path)
            self.live_data_feed.load_data()
            self.is_live_trading = True
            self.symbol = symbol
            print(f'âœ“ Started live trading for {symbol}')
            print(f'  Data source: {data_file_path}')
            print(f'  DWX directory: {self.dwx_transmitter.dwx_files_dir}')
            print(f'  Risk management: Active')
            return True
        except Exception as e:
            print(f'âœ— Failed to start live trading: {e}')
            return False

    def process_live_tick(self) ->Optional[Dict]:
        if not self.is_live_trading or not self.live_data_feed:
            return None
        try:
            tick = self.live_data_feed.get_current_tick()
            if not tick:
                return None
            current_state = self._create_v4_state(tick)
            action_vector = self.live_agent.get_action(current_state)
            action = action_vector.index(1)
            signal_info = self._process_v4_action(action, tick)
            return signal_info
        except Exception as e:
            print(f'Error processing live tick: {e}')
            return None

    def _create_v4_state(self, tick: Dict) ->np.ndarray:
        try:
            current_price = tick['close']
            recent_data = self.live_data_feed.data.tail(50)
            if len(recent_data) > 1:
                price_high = recent_data['high'].max()
                price_low = recent_data['low'].min()
                if price_high > price_low:
                    price_level = (current_price - price_low) / (price_high -
                        price_low)
                else:
                    price_level = 0.5
            else:
                price_level = 0.5
            if len(recent_data) > 5:
                returns = recent_data['close'].pct_change().dropna()
                volatility = returns.std() if len(returns) > 0 else 0.001
                volatility = min(volatility * 100, 5.0) / 5.0
            else:
                volatility = 0.5
            position_size = 1.0
            immediate_risk = 0.0
            if self.current_position != 0 and self.position_entry_price > 0:
                if self.current_position == 1:
                    unrealized_pnl_pct = (current_price - self.
                        position_entry_price) / self.position_entry_price
                else:
                    unrealized_pnl_pct = (self.position_entry_price -
                        current_price) / self.position_entry_price
                immediate_risk = unrealized_pnl_pct * 0.1
                immediate_risk = np.clip(immediate_risk, -0.2, 0.2)
            state = np.array([price_level, volatility, position_size,
                immediate_risk], dtype=float)
            return state
        except Exception as e:
            print(f'Error creating V4 state: {e}')
            return np.array([0.5, 0.5, 1.0, 0.0], dtype=float)

    def _process_v4_action(self, action: int, tick: Dict) ->Optional[Dict]:
        current_price = tick['close']
        signal_info = None
        is_valid, violations = self.risk_manager.validate_trade(self.symbol,
            'buy' if action == Direction.BUY else 'sell', 0.01, current_price)
        if not is_valid:
            print(f'Trade blocked by risk management: {violations}')
            return None
        target_position = action - 1
        if target_position != self.current_position:
            success = self._execute_position_change(target_position,
                current_price, tick['time'])
            if success:
                signal_info = {'timestamp': tick['time'], 'action': ['SELL',
                    'CLOSE', 'BUY'][action], 'previous_position': self.
                    current_position, 'new_position': target_position,
                    'price': current_price, 'risk_validated': True}
                self.signals_sent += 1
                self.last_signal_time = datetime.now()
        return signal_info

    def _execute_position_change(self, target_position: int, current_price:
        float, timestamp) ->bool:
        try:
            if self.current_position != 0:
                close_success = self.signal_generator.process_approach_signal(
                    'Pure RL', self.symbol, 0, lots=0.01)
                if close_success:
                    if self.position_entry_price > 0:
                        pnl = self._calculate_trade_pnl(current_price)
                        self.live_trades.append({'entry_price': self.
                            position_entry_price, 'exit_price':
                            current_price, 'direction': self.
                            current_position, 'pnl': pnl, 'timestamp':
                            timestamp})
                        self.risk_manager.update_balance(pnl)
            if target_position != 0:
                signal_success = self.signal_generator.process_approach_signal(
                    'Pure RL', self.symbol, target_position + 1, lots=0.01)
                if signal_success:
                    self.position_entry_price = current_price
                    self.current_position = target_position
                    self.risk_manager.update_position(self.symbol, 0.01,
                        current_price, 'buy' if target_position == 1 else
                        'sell')
                return signal_success
            else:
                self.current_position = 0
                self.position_entry_price = 0
                return True
        except Exception as e:
            print(f'Error executing position change: {e}')
            return False

    def _calculate_trade_pnl(self, exit_price: float) ->float:
        if self.position_entry_price == 0:
            return 0
        if self.current_position == 1:
            pnl_pct = (exit_price - self.position_entry_price
                ) / self.position_entry_price
        else:
            pnl_pct = (self.position_entry_price - exit_price
                ) / self.position_entry_price
        pnl_amount = pnl_pct * 0.01 * 100000 * self.position_entry_price
        return pnl_amount

    def get_live_performance(self) ->Dict:
        if not self.live_trades:
            return {'total_trades': 0, 'total_pnl': 0, 'win_rate': 0,
                'current_position': self.current_position, 'signals_sent':
                self.signals_sent}
        profitable_trades = [t for t in self.live_trades if t['pnl'] > 0]
        total_pnl = sum(t['pnl'] for t in self.live_trades)
        return {'total_trades': len(self.live_trades), 'total_pnl':
            total_pnl, 'win_rate': len(profitable_trades) / len(self.
            live_trades), 'avg_pnl': total_pnl / len(self.live_trades),
            'current_position': self.current_position, 'signals_sent': self
            .signals_sent, 'last_signal': self.last_signal_time,
            'risk_status': self.risk_manager.get_risk_metrics() if self.
            risk_manager else None}

    def stop_live_trading(self):
        if self.is_live_trading and self.current_position != 0:
            if self.live_data_feed:
                current_tick = self.live_data_feed.get_current_tick()
                if current_tick:
                    self._execute_position_change(0, current_tick['close'],
                        current_tick['time'])
        self.is_live_trading = False
        print('âœ“ Live trading stopped')

def main():
    print('V4 DWX Integration Demo')
    print('=' * 40)
    integration = V4DWXIntegration()
    model_path = './model/best_return_model.pth'
    if os.path.exists(model_path):
        integration.load_trained_model(model_path)
    else:
        print(f'Model not found: {model_path}')
        print('Train a model first using trainer.py')
        return
    data_path = '/home/hung/Public/Test/FX/data/GBPUSD60.csv'
    if integration.start_live_trading(data_path, 'GBPUSD'):
        print('\nProcessing live ticks (demo)...')
        for i in range(10):
            signal_info = integration.process_live_tick()
            if signal_info:
                print(f'Signal: {signal_info}')
            else:
                print(f'Tick {i + 1}: No action')
        perf = integration.get_live_performance()
        print(f'\nLive Performance: {perf}')
        integration.stop_live_trading()

if __name__ == '__main__':
    import torch
    main()

================================================
File: forex_game.py
================================================
import numpy as np
import pandas as pd
from datetime import datetime
from typing import Dict, Tuple
from collections import namedtuple
from config import V4Config

class Direction:
    CLOSE_FLAT = 0
    ENTER_LONG = 1
    ENTER_SHORT = 2

Position = namedtuple('Position',
    'direction entry_price entry_step position_size sl_price tp_price')

class HierarchicalForexGameAI:

    def __init__(self, data, initial_balance=10000):
        self.data = data
        self.initial_balance = initial_balance
        config = V4Config()
        self.risk_config = config.RISK_CONFIG
        self.risk_percent = self.risk_config['risk_percent_per_trade'] / 100.0
        self.leverage = self.risk_config['leverage']
        self.sl_distance_pips = self.risk_config['sl_distance_pips']
        self.tp_rr_ratio = self.risk_config['tp_rr_ratio']
        self.min_position_size = self.risk_config['min_position_size']
        self.max_position_size = self.risk_config['max_position_size']
        self.pip_value = self.risk_config['pip_value']
        self.commission_pips = self.risk_config['commission_pips']
        self.reset()

    def reset(self):
        self.current_step = 50
        self.balance = self.initial_balance
        self.max_balance = self.initial_balance
        self.position = None
        self.trades_history = []
        self.step_count = 0
        return self.get_h1_state()

    def get_h1_state(self):
        if self.current_step >= len(self.data):
            return np.array([0] * 13, dtype=float)
        current_price = self.data.iloc[self.current_step]['close']
        price_momentum = 0.0
        if self.current_step >= 5:
            price_5_ago = self.data.iloc[self.current_step - 5]['close']
            price_momentum = (current_price - price_5_ago) / price_5_ago
        recent_window = min(50, self.current_step)
        recent_data = self.data.iloc[self.current_step - recent_window:self
            .current_step + 1]
        price_high = recent_data['high'].max()
        price_low = recent_data['low'].min()
        if price_high > price_low:
            dist_high = (price_high - current_price) / (price_high - price_low)
            dist_low = (current_price - price_low) / (price_high - price_low)
        else:
            dist_high = 0.5
            dist_low = 0.5
        position_state = 0.0
        if self.position is not None:
            position_state = (1.0 if self.position.direction == Direction.
                ENTER_LONG else -1.0)
        unrealized_pnl = 0.0
        if self.position is not None:
            if self.position.direction == Direction.ENTER_LONG:
                unrealized_pnl = (current_price - self.position.entry_price
                    ) / self.position.entry_price
            else:
                unrealized_pnl = (self.position.entry_price - current_price
                    ) / self.position.entry_price
            unrealized_pnl = np.clip(unrealized_pnl, -0.2, 0.2)
        time_factor = self.current_step % 24 / 24.0
        volatility_20 = 0.001
        volatility_100 = 0.001
        if self.current_step >= 20:
            returns_20 = self.data.iloc[self.current_step - 20:self.
                current_step + 1]['close'].pct_change().dropna()
            if len(returns_20) > 0:
                volatility_20 = returns_20.std()
        if self.current_step >= 100:
            returns_100 = self.data.iloc[self.current_step - 100:self.
                current_step + 1]['close'].pct_change().dropna()
            if len(returns_100) > 0:
                volatility_100 = returns_100.std()
        volatility_regime = min(volatility_20 / volatility_100 if 
            volatility_100 > 0 else 1.0, 3.0) / 3.0
        trend_strength = 0.0
        if self.current_step >= 20:
            trend_data = self.data.iloc[self.current_step - 20:self.
                current_step + 1]['close']
            trend_strength = (trend_data.iloc[-1] - trend_data.iloc[0]
                ) / trend_data.iloc[0]
            trend_strength = np.clip(trend_strength, -0.1, 0.1) / 0.1
        risk_exposure = 0.0
        sl_distance = 0.0
        tp_distance = 0.0
        current_pips = 0.0
        if self.position is not None:
            risk_exposure = (self.position.position_size * current_price /
                self.balance)
            sl_distance = abs(current_price - self.position.sl_price
                ) / self.pip_value
            tp_distance = abs(self.position.tp_price - current_price
                ) / self.pip_value
            if self.position.direction == Direction.ENTER_LONG:
                current_pips = (current_price - self.position.entry_price
                    ) / self.pip_value
            else:
                current_pips = (self.position.entry_price - current_price
                    ) / self.pip_value
        current_candle = self.data.iloc[self.current_step]
        
        volume_ratio = self._calculate_volume_ratio()
        volume_price_confirm = self._calculate_volume_price_confirmation()
        
        session_indicator = self._get_session_indicator()
        
        hl_range = (current_candle['high'] - current_candle['low']) / current_candle['close']
        volatility_cluster = self._calculate_volatility_clustering()
        intrabar_momentum = self._calculate_intrabar_momentum()
        
        sr_distance = self._calculate_sr_distance()
        
        atr_value = self._calculate_atr()
        
        market_regime = self._detect_market_regime()
        
        state = np.array([
            np.clip(price_momentum, -0.05, 0.05) / 0.05,
            dist_high, 
            dist_low,
            np.clip(unrealized_pnl, -1.0, 1.0),
            volume_ratio,
            volume_price_confirm,
            session_indicator,
            np.clip(hl_range, 0, 0.05) / 0.05,
            volatility_cluster,
            intrabar_momentum,
            sr_distance,
            atr_value,
            market_regime
        ], dtype=float)
        return state

    def calculate_position_size(self, atr_value=None):
        if atr_value is None:
            atr_value = self._calculate_atr()
        
        atr_pips = atr_value * 10000
        sl_distance_pips = max(atr_pips * 2, 10)
        
        session = self._get_session_indicator()
        session_multiplier = 0.5 if session == 0.0 else 1.0
        
        risk_amount = self.balance * self.risk_percent * session_multiplier
        pip_value_per_lot = 10.0
        position_size = risk_amount / (sl_distance_pips * pip_value_per_lot)
        return np.clip(position_size, self.min_position_size, self.max_position_size)

    def set_sl_tp_levels(self, entry_price, direction):
        atr_value = self._calculate_atr()
        atr_pips = atr_value * 10000
        dynamic_sl_pips = max(atr_pips * 2, 10)
        dynamic_tp_pips = atr_pips * 3
        
        if direction == Direction.ENTER_LONG:
            sl_price = entry_price - dynamic_sl_pips * self.pip_value
            tp_price = entry_price + dynamic_tp_pips * self.pip_value
        else:
            sl_price = entry_price + dynamic_sl_pips * self.pip_value
            tp_price = entry_price - dynamic_tp_pips * self.pip_value
        return sl_price, tp_price

    def check_sl_tp_hit(self, current_price):
        if self.position is None:
            return False, 0
        if self.position.direction == Direction.ENTER_LONG:
            if current_price <= self.position.sl_price:
                return True, -1
            elif current_price >= self.position.tp_price:
                return True, 1
        elif current_price >= self.position.sl_price:
            return True, -1
        elif current_price <= self.position.tp_price:
            return True, 1
        return False, 0

    def play_step(self, action):
        self.step_count += 1
        reward = 0
        done = False
        if self.current_step >= len(self.data) - 1:
            done = True
            return reward, done, self.get_portfolio_value()
        current_price = self.data.iloc[self.current_step]['close']
        if self.position is not None:
            sl_tp_hit, hit_type = self.check_sl_tp_hit(current_price)
            if sl_tp_hit:
                reward = self._close_position_sl_tp(current_price, hit_type)
                self.current_step += 1
                return reward, done, self.get_portfolio_value()
        volatility_threshold = self._calculate_volatility_threshold()
        current_volatility = self._get_current_volatility()
        
        if current_volatility >= volatility_threshold:
            if action == Direction.ENTER_LONG:
                if self.position is None:
                    reward = self._execute_trade(current_price, Direction.ENTER_LONG)
                elif self.position.direction == Direction.ENTER_SHORT:
                    reward = self._close_position_manual(current_price)
                    reward += self._execute_trade(current_price, Direction.ENTER_LONG)
            elif action == Direction.ENTER_SHORT:
                if self.position is None:
                    reward = self._execute_trade(current_price, Direction.ENTER_SHORT)
                elif self.position.direction == Direction.ENTER_LONG:
                    reward = self._close_position_manual(current_price)
                    reward += self._execute_trade(current_price, Direction.ENTER_SHORT)
        
        if action == Direction.CLOSE_FLAT and self.position is not None:
            reward = self._close_position_manual(current_price)
        self.current_step += 1
        portfolio_value = self.get_portfolio_value()
        drawdown = (self.max_balance - portfolio_value) / self.max_balance
        if drawdown >= 0.2:
            done = True
            reward -= 50
        if portfolio_value < self.initial_balance * 0.5:
            done = True
            reward -= 50
        if portfolio_value > self.max_balance:
            self.max_balance = portfolio_value
        return reward, done, portfolio_value

    def _close_position_manual(self, current_price):
        if self.position is None:
            return 0
        
        if self.position.direction == Direction.ENTER_LONG:
            pnl_pct = (current_price - self.position.entry_price) / self.position.entry_price
        else:
            pnl_pct = (self.position.entry_price - current_price) / self.position.entry_price
        
        pnl_amount = pnl_pct * self.position.position_size * 100000 * self.position.entry_price
        self.balance += pnl_amount
        
        self.trades_history.append({
            'entry_price': self.position.entry_price,
            'exit_price': current_price,
            'direction': self.position.direction,
            'pnl_pct': pnl_pct,
            'pnl_amount': pnl_amount,
            'step': self.current_step,
            'hold_time': self.current_step - self.position.entry_step,
            'position_size': self.position.position_size,
            'exit_reason': 'MANUAL'
        })
        
        self.position = None
        return 25 if pnl_amount > 0 else -25

    def _execute_trade(self, current_price, new_direction):
        reward = 0
        position_size = self.calculate_position_size()
        sl_price, tp_price = self.set_sl_tp_levels(current_price, new_direction
            )
        self.position = Position(new_direction, current_price, self.
            current_step, position_size, sl_price, tp_price)
        commission = self.commission_pips * (position_size * 10.0)
        self.balance -= commission
        direction_name = ('LONG' if new_direction == Direction.ENTER_LONG else
            'SHORT')
        print(
            f'ðŸ”µ TRADE ENTRY: {direction_name} @ ${current_price:.5f} | Size: {position_size:.2f} lots | SL: ${sl_price:.5f} | TP: ${tp_price:.5f} | Risk: ${position_size * current_price:.2f}'
            )
        return reward
    
    def _calculate_volatility_threshold(self):
        if self.current_step >= 100:
            returns_100 = self.data.iloc[self.current_step - 100:self.current_step + 1]['close'].pct_change().dropna()
            if len(returns_100) > 0:
                return returns_100.std() * 0.5
        return 0.001
    
    def _get_current_volatility(self):
        if self.current_step >= 20:
            returns_20 = self.data.iloc[self.current_step - 20:self.current_step + 1]['close'].pct_change().dropna()
            if len(returns_20) > 0:
                return returns_20.std()
        return 0.001

    def _close_position_sl_tp(self, current_price, hit_type):
        if self.position is None:
            return 0
        if self.position.direction == Direction.ENTER_LONG:
            pnl_pct = (current_price - self.position.entry_price
                ) / self.position.entry_price
        else:
            pnl_pct = (self.position.entry_price - current_price
                ) / self.position.entry_price
        pnl_amount = (pnl_pct * self.position.position_size * 100000 * self
            .position.entry_price)
        self.balance += pnl_amount
        direction_name = ('LONG' if self.position.direction == Direction.
            ENTER_LONG else 'SHORT')
        hit_reason = 'ðŸ›‘ STOP LOSS' if hit_type == -1 else 'ðŸŽ¯ TAKE PROFIT'
        pips = abs(current_price - self.position.entry_price) / self.pip_value
        print(
            f'ðŸ”´ {hit_reason}: {direction_name} @ ${current_price:.5f} | Entry: ${self.position.entry_price:.5f} | {pips:.1f} pips | P&L: ${pnl_amount:.2f}'
            )
        reward = -100 if hit_type == -1 else 100
        self.trades_history.append({'entry_price': self.position.
            entry_price, 'exit_price': current_price, 'direction': self.
            position.direction, 'pnl_pct': pnl_pct, 'pnl_amount':
            pnl_amount, 'step': self.current_step, 'hold_time': self.
            current_step - self.position.entry_step, 'position_size': self.
            position.position_size, 'exit_reason': 'SL' if hit_type == -1 else
            'TP'})
        self.position = None
        return reward

    def get_portfolio_value(self):
        portfolio_value = self.balance
        if self.position is not None and self.current_step < len(self.data):
            current_price = self.data.iloc[self.current_step]['close']
            if self.position.direction == Direction.ENTER_LONG:
                unrealized_pnl_pct = (current_price - self.position.entry_price
                    ) / self.position.entry_price
            else:
                unrealized_pnl_pct = (self.position.entry_price - current_price
                    ) / self.position.entry_price
            unrealized_pnl = (unrealized_pnl_pct * self.position.
                position_size * 100000 * self.position.entry_price)
            portfolio_value += unrealized_pnl
        return portfolio_value

    def get_current_price(self):
        if self.current_step < len(self.data):
            return self.data.iloc[self.current_step]['close']
        return 0

    def get_trade_stats(self):
        if not self.trades_history:
            return {'total_trades': 0, 'win_rate': 0, 'avg_pnl': 0,
                'total_pnl': 0, 'avg_hold_time': 0}
        profitable_trades = [t for t in self.trades_history if t[
            'pnl_amount'] > 0]
        return {'total_trades': len(self.trades_history), 'win_rate': len(
            profitable_trades) / len(self.trades_history), 'avg_pnl': np.
            mean([t['pnl_amount'] for t in self.trades_history]),
            'total_pnl': sum([t['pnl_amount'] for t in self.trades_history]
            ), 'avg_hold_time': np.mean([t['hold_time'] for t in self.
            trades_history])}
    
    def _calculate_volume_ratio(self):
        if self.current_step < 20 or 'volume' not in self.data.columns:
            return 0.5
        current_volume = self.data.iloc[self.current_step]['volume']
        avg_volume = self.data.iloc[self.current_step - 20:self.current_step + 1]['volume'].mean()
        if avg_volume > 0:
            ratio = current_volume / avg_volume
            return np.clip(ratio / 3.0, 0, 1.0)
        return 0.5
    
    def _calculate_volume_price_confirmation(self):
        if self.current_step < 2 or 'volume' not in self.data.columns:
            return 0.0
        
        current = self.data.iloc[self.current_step]
        previous = self.data.iloc[self.current_step - 1]
        
        price_direction = 1 if current['close'] > previous['close'] else -1
        volume_direction = 1 if current['volume'] > previous['volume'] else -1
        
        return price_direction * volume_direction * 0.5
    
    def _get_session_indicator(self):
        if 'time' not in self.data.columns:
            return 0.0
        
        try:
            time_str = str(self.data.iloc[self.current_step]['time'])
            if ':' in time_str:
                hour = int(time_str.split(' ')[1].split(':')[0]) if ' ' in time_str else int(time_str.split(':')[0])
            else:
                hour = self.current_step % 24
        except:
            hour = self.current_step % 24
        
        if 21 <= hour or hour < 5:
            return 0.0
        elif 8 <= hour < 16:
            return 1.0
        elif 15 <= hour < 23:
            return 2.0 if hour == 15 else 1.5
        else:
            return 0.5
    
    def _calculate_volatility_clustering(self):
        if self.current_step < 20:
            return 0.5
        
        current_hl = self.data.iloc[self.current_step]['high'] - self.data.iloc[self.current_step]['low']
        avg_hl = np.mean([
            self.data.iloc[i]['high'] - self.data.iloc[i]['low'] 
            for i in range(max(0, self.current_step - 20), self.current_step)
        ])
        
        if avg_hl > 0:
            cluster = current_hl / avg_hl
            return np.clip(cluster / 3.0, 0, 1.0)
        return 0.5
    
    def _calculate_intrabar_momentum(self):
        current = self.data.iloc[self.current_step]
        hl_range = current['high'] - current['low']
        
        if hl_range > 0:
            momentum = (current['close'] - current['open']) / hl_range
            return np.clip(momentum, -1.0, 1.0)
        return 0.0
    
    def _calculate_sr_distance(self):
        if self.current_step < 50:
            return 0.5
        
        lookback = min(50, self.current_step)
        data_window = self.data.iloc[self.current_step - lookback:self.current_step + 1]
        
        highs = []
        lows = []
        
        for i in range(2, len(data_window) - 2):
            if (data_window.iloc[i]['high'] > data_window.iloc[i-1]['high'] and 
                data_window.iloc[i]['high'] > data_window.iloc[i-2]['high'] and
                data_window.iloc[i]['high'] > data_window.iloc[i+1]['high'] and
                data_window.iloc[i]['high'] > data_window.iloc[i+2]['high']):
                highs.append(data_window.iloc[i]['high'])
            
            if (data_window.iloc[i]['low'] < data_window.iloc[i-1]['low'] and 
                data_window.iloc[i]['low'] < data_window.iloc[i-2]['low'] and
                data_window.iloc[i]['low'] < data_window.iloc[i+1]['low'] and
                data_window.iloc[i]['low'] < data_window.iloc[i+2]['low']):
                lows.append(data_window.iloc[i]['low'])
        
        current_price = self.data.iloc[self.current_step]['close']
        
        if not highs and not lows:
            return 0.5
        
        distances = []
        if highs:
            distances.extend([abs(current_price - h) for h in highs])
        if lows:
            distances.extend([abs(current_price - l) for l in lows])
        
        if distances:
            min_distance = min(distances) / self.pip_value
            return np.clip(min_distance / 100.0, 0, 1.0)
        
        return 0.5
    
    def _calculate_atr(self):
        if self.current_step < 14:
            return 0.001
        
        atr_values = []
        for i in range(max(0, self.current_step - 14), self.current_step):
            current = self.data.iloc[i]
            previous = self.data.iloc[i-1] if i > 0 else current
            
            tr = max(
                current['high'] - current['low'],
                abs(current['high'] - previous['close']),
                abs(current['low'] - previous['close'])
            )
            atr_values.append(tr)
        
        if atr_values:
            atr = np.mean(atr_values)
            return np.clip(atr / current['close'], 0, 0.01) * 100
        
        return 0.001
    
    def _detect_market_regime(self):
        if self.current_step < 50:
            return 0.5
        
        window = min(50, self.current_step)
        prices = [self.data.iloc[i]['close'] for i in range(max(0, self.current_step - window), self.current_step)]
        
        if len(prices) < 20:
            return 0.5
        
        bb_period = 20
        bb_prices = prices[-bb_period:] if len(prices) >= bb_period else prices
        bb_mean = np.mean(bb_prices)
        bb_std = np.std(bb_prices)
        
        if bb_std == 0:
            return 0.5
        
        current_price = prices[-1]
        bb_upper = bb_mean + 2 * bb_std
        bb_lower = bb_mean - 2 * bb_std
        bb_width = (bb_upper - bb_lower) / bb_mean
        
        trend_lookback = min(20, len(prices))
        if trend_lookback >= 10:
            first_half = np.mean(prices[-trend_lookback:-trend_lookback//2])
            second_half = np.mean(prices[-trend_lookback//2:])
            trend_strength = abs(second_half - first_half) / first_half
        else:
            trend_strength = 0
        
        if bb_width < 0.02 and trend_strength < 0.01:
            return 0.0
        elif trend_strength > 0.03:
            return 1.0
        else:
            return 0.5

ForexGameAI = HierarchicalForexGameAI

================================================
File: grid_search_config.py
================================================
import itertools
import random
import json
from datetime import datetime
import os

class V4GridSearchConfig:

    def __init__(self, max_neurons_per_layer=55):
        self.max_neurons = max_neurons_per_layer
        self.results = []

    def generate_layer_combinations(self):
        combinations = []
        for n1 in range(8, self.max_neurons + 1, 4):
            combinations.append([n1])
        for n1 in range(16, self.max_neurons + 1, 8):
            for n2 in range(8, n1, 4):
                combinations.append([n1, n2])
        for n1 in range(32, self.max_neurons + 1, 8):
            for n2 in range(16, n1, 8):
                for n3 in range(8, n2, 4):
                    combinations.append([n1, n2, n3])
        for n1 in range(40, self.max_neurons + 1, 8):
            for n2 in range(24, n1, 8):
                for n3 in range(16, n2, 8):
                    for n4 in range(8, n3, 4):
                        combinations.append([n1, n2, n3, n4])
        print(f'Generated {len(combinations)} layer combinations')
        return combinations

    def generate_training_combinations(self):
        learning_rates = [0.01, 0.001, 0.0001]
        gammas = [0.9, 0.95, 0.99]
        epsilon_decays = [0.995, 0.9995, 0.9999]
        batch_sizes = [512, 1000, 2000]
        combinations = list(itertools.product(learning_rates, gammas,
            epsilon_decays, batch_sizes))
        print(f'Generated {len(combinations)} training combinations')
        return combinations

    def create_config(self, hidden_layers, lr, gamma, epsilon_decay, batch_size
        ):
        return {'state_size': 8, 'action_size': 3, 'network': {
            'hidden_layers': hidden_layers, 'dropout_rate': 0.2,
            'activation': 'relu', 'batch_norm': True}, 'training': {'lr':
            lr, 'gamma': gamma, 'epsilon_start': 80, 'epsilon_decay':
            epsilon_decay, 'epsilon_min': 0.01, 'memory_size': 100000,
            'batch_size': batch_size, 'target_update_freq': 1000},
            'advanced': {'double_dqn': True, 'dueling_dqn': True,
            'prioritized_replay': True, 'noisy_networks': False,
            'multi_step': 3, 'gradient_clipping': 1.0}, 'risk': {
            'use_risk_features': True, 'adaptive_position_sizing': False,
            'risk_based_rewards': True, 'stop_loss_integration': True},
            'gpu': {'use_gpu': True, 'mixed_precision': True,
            'data_parallel': True, 'device_ids': [0, 1]}}

    def generate_search_space(self, max_configs=50):
        layer_combos = self.generate_layer_combinations()
        training_combos = self.generate_training_combinations()
        if len(layer_combos) * len(training_combos) > max_configs:
            layer_sample = random.sample(layer_combos, min(len(layer_combos
                ), max_configs // 4))
            training_sample = random.sample(training_combos, min(len(
                training_combos), 4))
        else:
            layer_sample = layer_combos
            training_sample = training_combos
        configs = []
        config_id = 0
        for layers in layer_sample:
            for lr, gamma, epsilon_decay, batch_size in training_sample:
                config = self.create_config(layers, lr, gamma,
                    epsilon_decay, batch_size)
                config['config_id'] = config_id
                config['config_name'] = (
                    f"config_{config_id:03d}_layers_{'_'.join(map(str, layers))}"
                    )
                configs.append(config)
                config_id += 1
                if len(configs) >= max_configs:
                    break
            if len(configs) >= max_configs:
                break
        print(f'Generated {len(configs)} configurations for testing')
        return configs

    def save_results(self, results, filename=None):
        if filename is None:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'grid_search_results_{timestamp}.json'
        with open(filename, 'w') as f:
            json.dump(results, f, indent=2)
        print(f'Results saved to {filename}')

    def analyze_results(self, results):
        if not results:
            print('No results to analyze')
            return
        sorted_by_return = sorted(results, key=lambda x: x.get(
            'final_return', -999), reverse=True)
        sorted_by_winrate = sorted(results, key=lambda x: x.get('win_rate',
            0), reverse=True)
        sorted_by_sharpe = sorted(results, key=lambda x: x.get(
            'sharpe_ratio', -999), reverse=True)
        print('\n' + '=' * 80)
        print('GRID SEARCH RESULTS ANALYSIS')
        print('=' * 80)
        print('\nTOP 5 BY TOTAL RETURN:')
        for i, config in enumerate(sorted_by_return[:5]):
            layers = config.get('network', {}).get('hidden_layers', [])
            lr = config.get('training', {}).get('lr', 0)
            print(f"{i + 1}. {config.get('config_name', 'unknown')}")
            print(
                f"   Layers: {layers} | LR: {lr} | Return: {config.get('final_return', 0):.2f}%"
                )
            print(
                f"   Win Rate: {config.get('win_rate', 0):.2%} | Trades: {config.get('total_trades', 0)}"
                )
        print('\nTOP 5 BY WIN RATE:')
        for i, config in enumerate(sorted_by_winrate[:5]):
            layers = config.get('network', {}).get('hidden_layers', [])
            lr = config.get('training', {}).get('lr', 0)
            print(f"{i + 1}. {config.get('config_name', 'unknown')}")
            print(
                f"   Layers: {layers} | LR: {lr} | Win Rate: {config.get('win_rate', 0):.2%}"
                )
            print(
                f"   Return: {config.get('final_return', 0):.2f}% | Trades: {config.get('total_trades', 0)}"
                )
        layer_performance = {}
        for result in results:
            layers = tuple(result.get('network', {}).get('hidden_layers', []))
            if layers not in layer_performance:
                layer_performance[layers] = []
            layer_performance[layers].append(result.get('final_return', 0))
        avg_performance = {layers: (sum(returns) / len(returns)) for layers,
            returns in layer_performance.items()}
        best_layers = sorted(avg_performance.items(), key=lambda x: x[1],
            reverse=True)
        print(f'\nBEST LAYER CONFIGURATIONS (by average return):')
        for i, (layers, avg_return) in enumerate(best_layers[:10]):
            param_count = self.calculate_parameters(list(layers))
            print(
                f'{i + 1}. {list(layers)} | Avg Return: {avg_return:.2f}% | Params: {param_count:,}'
                )
        return sorted_by_return[0] if sorted_by_return else None

    def calculate_parameters(self, hidden_layers):
        input_size = 8
        output_size = 3
        total_params = 0
        prev_size = input_size
        for layer_size in hidden_layers:
            total_params += prev_size * layer_size + layer_size
            prev_size = layer_size
        total_params += prev_size * output_size + output_size
        return total_params

def create_optimal_configs():
    return {'minimal': {'hidden_layers': [16, 8], 'params': 16 * 8 + 8 + 8 *
        8 + 8 + 8 * 3 + 3}, 'small': {'hidden_layers': [32, 16], 'params': 
        32 * 8 + 32 + 16 * 32 + 16 + 16 * 3 + 3}, 'medium': {
        'hidden_layers': [48, 24, 12], 'params': 48 * 8 + 48 + 24 * 48 + 24 +
        12 * 24 + 12 + 12 * 3 + 3}, 'balanced': {'hidden_layers': [55, 32, 
        16], 'params': 55 * 8 + 55 + 32 * 55 + 32 + 16 * 32 + 16 + 16 * 3 + 3}}

if __name__ == '__main__':
    grid_search = V4GridSearchConfig(max_neurons_per_layer=55)
    configs = grid_search.generate_search_space(max_configs=20)
    print('\nSample configurations generated:')
    for i, config in enumerate(configs[:5]):
        layers = config['network']['hidden_layers']
        lr = config['training']['lr']
        params = grid_search.calculate_parameters(layers)
        print(f"{i + 1}. {config['config_name']}")
        print(f'   Layers: {layers} | LR: {lr} | Parameters: {params:,}')
    with open('grid_search_configs.json', 'w') as f:
        json.dump(configs, f, indent=2)
    print(
        f'\nâœ“ Saved {len(configs)} configurations to grid_search_configs.json')
    print('âœ“ Ready for automated testing!')
    optimal = create_optimal_configs()
    print(f'\nHand-picked efficient configurations:')
    for name, info in optimal.items():
        print(
            f"  {name}: {info['hidden_layers']} ({info['params']:,} parameters)"
            )

================================================
File: grid_search_trainer.py
================================================
import torch
import pandas as pd
import numpy as np
import json
import time
from datetime import datetime
import os
from grid_search_config import V4GridSearchConfig
from agent import Agent
from forex_game import ForexGameAI

class GridSearchTrainer:

    def __init__(self, data_path=None):
        self.data_path = data_path
        self.results = []
        self.best_config = None
        self.best_performance = -999

    def load_data(self):
        if not self.data_path:
            from real_data_loader import load_real_gbpusd_data
            try:
                data, _ = load_real_gbpusd_data()
                print(f'Loaded real GBPUSD data: {len(data)} candles')
                return data
            except Exception as e:
                raise ValueError(f'No data path provided and real data loading failed: {e}')
        
        if not os.path.exists(self.data_path):
            raise FileNotFoundError(f'Data file not found: {self.data_path}')
        
        df = pd.read_csv(self.data_path)
        print(f'Loaded {len(df)} candles from {self.data_path}')
        return df

    def test_single_config(self, config, max_episodes=100):
        print(f"\nTesting {config['config_name']}")
        print(
            f"Layers: {config['network']['hidden_layers']} | LR: {config['training']['lr']}"
            )
        data = self.load_data()
        agent = Agent()
        agent.model.config = config
        agent.memory_size = config['training']['memory_size']
        agent.batch_size = config['training']['batch_size']
        agent.epsilon = config['training']['epsilon_start']
        agent.epsilon_decay_rate = config['training']['epsilon_decay']
        agent.epsilon_min = config['training']['epsilon_min']
        game = ForexGameAI(data)
        scores = []
        portfolio_values = []
        start_time = time.time()
        for episode in range(max_episodes):
            game.reset()
            episode_score = 0
            while True:
                state = agent.get_state(game)
                action = agent.get_action(state)
                reward, done, portfolio_value = game.play_step(action.index(1))
                next_state = agent.get_state(game)
                agent.train_short_memory(state, action, reward, next_state,
                    done)
                agent.remember(state, action, reward, next_state, done)
                episode_score += reward
                if done:
                    break
            agent.train_long_memory()
            scores.append(episode_score)
            portfolio_values.append(portfolio_value)
            if episode % 20 == 0:
                avg_score = np.mean(scores[-10:]) if len(scores
                    ) >= 10 else np.mean(scores)
                print(
                    f'Episode {episode:3d} | Score: {episode_score:6.1f} | Portfolio: ${portfolio_value:8.2f} | Avg: {avg_score:6.1f}'
                    )
        training_time = time.time() - start_time
        final_portfolio = portfolio_values[-1] if portfolio_values else 10000
        final_return = (final_portfolio - 10000) / 10000 * 100
        stats = game.get_trade_stats()
        result = {'config_id': config['config_id'], 'config_name': config[
            'config_name'], 'network': config['network'], 'training':
            config['training'], 'final_portfolio': final_portfolio,
            'final_return': final_return, 'total_trades': stats.get(
            'total_trades', 0), 'win_rate': stats.get('win_rate', 0),
            'avg_pnl': stats.get('avg_pnl', 0), 'total_pnl': stats.get(
            'total_pnl', 0), 'max_drawdown': stats.get('max_drawdown', 0),
            'training_time': training_time, 'episodes_completed': max_episodes}
        print(
            f"Final Result: Return={final_return:.2f}% | Trades={stats.get('total_trades', 0)} | Win Rate={stats.get('win_rate', 0):.2%}"
            )
        return result

    def run_grid_search(self, max_configs=20, episodes_per_config=50):
        print('Starting Grid Search for V4 DQN Architecture')
        print('=' * 60)
        grid_config = V4GridSearchConfig(max_neurons_per_layer=55)
        configs = grid_config.generate_search_space(max_configs=max_configs)
        all_results = []
        for i, config in enumerate(configs):
            print(f'\n[{i + 1}/{len(configs)}] Testing Configuration')
            try:
                result = self.test_single_config(config, max_episodes=
                    episodes_per_config)
                all_results.append(result)
                if result['final_return'] > self.best_performance:
                    self.best_performance = result['final_return']
                    self.best_config = config
                    print(f"ðŸ† NEW BEST: {result['final_return']:.2f}% return!")
            except Exception as e:
                print(f'âŒ Config failed: {e}')
                continue
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        results_file = f'grid_search_results_{timestamp}.json'
        with open(results_file, 'w') as f:
            json.dump(all_results, f, indent=2)
        print(f"\n{'=' * 60}")
        print('GRID SEARCH COMPLETED')
        print(f"{'=' * 60}")
        print(f'Tested {len(all_results)} configurations')
        print(f'Results saved to: {results_file}')
        if all_results:
            grid_config.analyze_results(all_results)
            if self.best_config:
                print(f'\nðŸ† BEST CONFIGURATION:')
                print(f"Name: {self.best_config['config_name']}")
                print(f"Layers: {self.best_config['network']['hidden_layers']}"
                    )
                print(f"LR: {self.best_config['training']['lr']}")
                print(f'Performance: {self.best_performance:.2f}% return')
                best_config_file = f'best_config_{timestamp}.json'
                with open(best_config_file, 'w') as f:
                    json.dump(self.best_config, f, indent=2)
                print(f'Best config saved to: {best_config_file}')

def main():
    real_data_path = '/home/hung/Public/Test/FX/data/GBPUSD60.csv'
    trainer = GridSearchTrainer(real_data_path)
    trainer.run_grid_search(max_configs=15, episodes_per_config=30)

if __name__ == '__main__':
    main()
...