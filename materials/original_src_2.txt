
================================================
File: gym_env_adapter.py
================================================
import gymnasium as gym
import numpy as np
import pandas as pd
from typing import Dict, Tuple, Any
import warnings
warnings.filterwarnings('ignore')
try:
    import gym_trading_env
    GYM_TRADING_ENV_AVAILABLE = True
except ImportError:
    print('gym-trading-env not installed. Run: pip install gym-trading-env')
    GYM_TRADING_ENV_AVAILABLE = False

class V4GymTradingEnv:

    def __init__(self, df: pd.DataFrame, initial_portfolio_value: float=10000):
        if not GYM_TRADING_ENV_AVAILABLE:
            raise ImportError('gym-trading-env is required but not installed')
        self.df = self._prepare_dataframe(df)
        self.initial_portfolio_value = initial_portfolio_value
        self.env = gym.make('TradingEnv', df=self.df, positions=[0, 1],
            trading_fees=0.0001, borrow_interest_rate=0,
            portfolio_initial_value=initial_portfolio_value,
            initial_position=0, max_episode_duration='max', verbose=0, name
            ='V4_Forex')
        self.current_position = 0
        self.position_entry_price = 0
        self.step_count = 0
        print(
            f'‚úì V4 Gym Environment initialized with {len(self.df)} data points'
            )

    def _prepare_dataframe(self, df: pd.DataFrame) ->pd.DataFrame:
        prepared_df = df.copy()
        required_cols = ['open', 'high', 'low', 'close', 'volume']
        for col in required_cols:
            if col not in prepared_df.columns:
                if col == 'volume':
                    prepared_df['volume'] = 1000
                else:
                    raise ValueError(
                        f"Required column '{col}' not found in dataframe")
        if not isinstance(prepared_df.index, pd.DatetimeIndex):
            if 'time' in prepared_df.columns:
                prepared_df['time'] = pd.to_datetime(prepared_df['time'])
                prepared_df.set_index('time', inplace=True)
            else:
                prepared_df.index = pd.date_range(start='2020-01-01',
                    periods=len(prepared_df), freq='H')
        prepared_df['feature_close'] = prepared_df['close'].pct_change(
            ).fillna(0)
        return prepared_df

    def reset(self) ->np.ndarray:
        obs, info = self.env.reset()
        self.current_position = 0
        self.position_entry_price = 0
        self.step_count = 0
        return self._convert_to_v4_state(obs, info)

    def step(self, action: int) ->Tuple[np.ndarray, float, bool, bool, Dict]:
        gym_action = self._convert_v4_action_to_gym(action)
        obs, reward, done, truncated, info = self.env.step(gym_action)
        v4_reward = self._convert_to_v4_reward(reward, info)
        v4_state = self._convert_to_v4_state(obs, info)
        self.step_count += 1
        return v4_state, v4_reward, done, truncated, info

    def _convert_v4_action_to_gym(self, v4_action: int) ->int:
        if v4_action == 1:
            return 1
        else:
            return 0

    def _convert_to_v4_state(self, obs, info) ->np.ndarray:
        try:
            current_price = info.get('data_close', self.df['close'].iloc[-1])
            lookback = min(50, len(self.df))
            recent_data = self.df.tail(lookback)
            price_high = recent_data['high'].max()
            price_low = recent_data['low'].min()
            if price_high > price_low:
                price_level = (current_price - price_low) / (price_high -
                    price_low)
            else:
                price_level = 0.5
            if len(recent_data) > 5:
                returns = recent_data['close'].pct_change().dropna()
                volatility = returns.std() if len(returns) > 0 else 0.001
                volatility = min(volatility * 100, 5.0) / 5.0
            else:
                volatility = 0.5
            position_size = 1.0
            portfolio_value = info.get('portfolio_valuation', self.
                initial_portfolio_value)
            immediate_risk = (portfolio_value - self.initial_portfolio_value
                ) / self.initial_portfolio_value
            immediate_risk = np.clip(immediate_risk, -0.2, 0.2)
            state = np.array([price_level, volatility, position_size,
                immediate_risk], dtype=float)
            state = np.nan_to_num(state, nan=0.5)
            return state
        except Exception as e:
            print(f'Error converting to V4 state: {e}')
            return np.array([0.5, 0.5, 1.0, 0.0], dtype=float)

    def _convert_to_v4_reward(self, gym_reward: float, info: Dict) ->float:
        portfolio_value = info.get('portfolio_valuation', self.
            initial_portfolio_value)
        current_gym_position = info.get('position', 0)
        if gym_reward > 0.001:
            return 10.0
        elif gym_reward < -0.001:
            return -10.0
        else:
            return 0.0

    def get_portfolio_info(self) ->Dict:
        try:
            dummy_obs, dummy_reward, dummy_done, dummy_truncated, info = (self
                .env.step(0))
            return {'portfolio_value': info.get('portfolio_valuation', self
                .initial_portfolio_value), 'position': info.get('position',
                0), 'real_position': info.get('real_position', 0), 'step':
                info.get('step', self.step_count)}
        except:
            return {'portfolio_value': self.initial_portfolio_value,
                'position': 0, 'real_position': 0, 'step': self.step_count}

    def render(self):
        try:
            return self.env.render()
        except:
            pass

class V4GymTrainer:

    def __init__(self, data_path: str=None):
        self.data_path = data_path
        self.env = None

    def create_environment(self, df: pd.DataFrame=None) ->V4GymTradingEnv:
        if df is None:
            df = self._load_or_generate_data()
        self.env = V4GymTradingEnv(df)
        return self.env

    def _load_or_generate_data(self) ->pd.DataFrame:
        if self.data_path and os.path.exists(self.data_path):
            try:
                df = pd.read_csv(self.data_path)
                print(f'Loaded data from {self.data_path}: {len(df)} rows')
                return df
            except Exception as e:
                print(f'Error loading data: {e}')
        from real_data_loader import load_real_gbpusd_data
        try:
            data, _ = load_real_gbpusd_data()
            print(f'Loaded real GBPUSD data for gym environment: {len(data)} candles')
            return data
        except Exception as e:
            raise ValueError(f'No data path provided and real data loading failed: {e}')

    def train_with_gym_env(self, episodes: int=100) ->Dict:
        if not self.env:
            raise ValueError(
                'Environment not created. Call create_environment() first.')
        print(f'Training with gym-trading-env backend for {episodes} episodes')
        total_rewards = []
        portfolio_values = []
        for episode in range(episodes):
            state = self.env.reset()
            episode_reward = 0
            steps = 0
            while True:
                action = np.random.randint(0, 3)
                next_state, reward, done, truncated, info = self.env.step(
                    action)
                episode_reward += reward
                steps += 1
                if done or truncated:
                    break
                state = next_state
            portfolio_info = self.env.get_portfolio_info()
            portfolio_value = portfolio_info['portfolio_value']
            total_rewards.append(episode_reward)
            portfolio_values.append(portfolio_value)
            if episode % 10 == 0:
                avg_reward = np.mean(total_rewards[-10:])
                avg_portfolio = np.mean(portfolio_values[-10:])
                print(
                    f'Episode {episode}: Avg Reward: {avg_reward:.2f}, Avg Portfolio: ${avg_portfolio:.2f}'
                    )
        return {'total_rewards': total_rewards, 'portfolio_values':
            portfolio_values, 'final_portfolio': portfolio_values[-1] if
            portfolio_values else 10000}

def main():
    if not GYM_TRADING_ENV_AVAILABLE:
        print(
            'gym-trading-env not available. Install with: pip install gym-trading-env'
            )
        return
    print('V4 Gym Trading Environment Demo')
    print('=' * 40)
    trainer = V4GymTrainer()
    env = trainer.create_environment()
    results = trainer.train_with_gym_env(episodes=50)
    print(f'\nDemo Results:')
    print(f"Final Portfolio: ${results['final_portfolio']:.2f}")
    print(f"Return: {(results['final_portfolio'] - 10000) / 10000 * 100:.2f}%")

if __name__ == '__main__':
    import os
    main()

================================================
File: helper.py
================================================
import matplotlib.pyplot as plt
from IPython import display
import numpy as np
plt.ion()

def plot(scores, mean_scores, portfolio_values=None):
    display.clear_output(wait=True)
    display.display(plt.gcf())
    plt.clf()
    if portfolio_values is not None:
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 8))
        ax1.plot(portfolio_values, 'b-', label='Portfolio Value')
        ax1.axhline(y=10000, color='gray', linestyle='--', alpha=0.7, label
            ='Initial Balance')
        ax1.set_title('Portfolio Value Progress')
        ax1.set_ylabel('Portfolio Value ($)')
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        if len(portfolio_values) > 0:
            ax1.text(len(portfolio_values) - 1, portfolio_values[-1],
                f'${portfolio_values[-1]:.0f}', ha='right', va='bottom')
        ax2.plot(scores, 'g-', label='Return %')
        ax2.plot(mean_scores, 'r-', label='Average Return %')
        ax2.axhline(y=0, color='gray', linestyle='--', alpha=0.7)
        ax2.set_title('Returns Progress')
        ax2.set_xlabel('Episode')
        ax2.set_ylabel('Return (%)')
        ax2.legend()
        ax2.grid(True, alpha=0.3)
        if len(scores) > 0:
            ax2.text(len(scores) - 1, scores[-1], f'{scores[-1]:.1f}%', ha=
                'right', va='bottom')
        if len(mean_scores) > 0:
            ax2.text(len(mean_scores) - 1, mean_scores[-1],
                f'{mean_scores[-1]:.1f}%', ha='right', va='top')
    else:
        plt.title('Training Progress')
        plt.xlabel('Number of Games')
        plt.ylabel('Return (%)')
        plt.plot(scores, 'b-', label='Return')
        plt.plot(mean_scores, 'r-', label='Average Return')
        plt.ylim(ymin=-50, ymax=50)
        if len(scores) > 0:
            plt.text(len(scores) - 1, scores[-1], str(f'{scores[-1]:.1f}%'))
        if len(mean_scores) > 0:
            plt.text(len(mean_scores) - 1, mean_scores[-1], str(
                f'{mean_scores[-1]:.1f}%'))
        plt.legend()
        plt.grid(True, alpha=0.3)
    plt.tight_layout()
    plt.show(block=False)
    plt.pause(0.1)

def plot_trade_analysis(trades_history):
    if not trades_history:
        print('No trades to analyze')
        return
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
    pnl_amounts = [trade['pnl_amount'] for trade in trades_history]
    pnl_pcts = [(trade['pnl_pct'] * 100) for trade in trades_history]
    directions = [trade['direction'] for trade in trades_history]
    ax1.hist(pnl_amounts, bins=20, alpha=0.7, color='blue', edgecolor='black')
    ax1.axvline(x=0, color='red', linestyle='--', alpha=0.7)
    ax1.set_title('P&L Distribution')
    ax1.set_xlabel('P&L Amount ($)')
    ax1.set_ylabel('Frequency')
    ax1.grid(True, alpha=0.3)
    cumulative_pnl = np.cumsum(pnl_amounts)
    ax2.plot(cumulative_pnl, 'g-', linewidth=2)
    ax2.axhline(y=0, color='red', linestyle='--', alpha=0.7)
    ax2.set_title('Cumulative P&L')
    ax2.set_xlabel('Trade Number')
    ax2.set_ylabel('Cumulative P&L ($)')
    ax2.grid(True, alpha=0.3)
    wins_losses = [(1 if pnl > 0 else -1) for pnl in pnl_amounts]
    ax3.bar(range(len(wins_losses)), wins_losses, color=[('green' if w > 0 else
        'red') for w in wins_losses])
    ax3.set_title('Win/Loss Pattern')
    ax3.set_xlabel('Trade Number')
    ax3.set_ylabel('Win (+1) / Loss (-1)')
    ax3.grid(True, alpha=0.3)
    profitable_trades = len([p for p in pnl_amounts if p > 0])
    win_rate = profitable_trades / len(pnl_amounts) * 100
    avg_win = np.mean([p for p in pnl_amounts if p > 0]
        ) if profitable_trades > 0 else 0
    avg_loss = np.mean([p for p in pnl_amounts if p < 0]) if len(pnl_amounts
        ) - profitable_trades > 0 else 0
    stats_text = f"""Trade Statistics:
Total Trades: {len(trades_history)}
Win Rate: {win_rate:.1f}%
Avg Win: ${avg_win:.2f}
Avg Loss: ${avg_loss:.2f}
Total P&L: ${sum(pnl_amounts):.2f}
Best Trade: ${max(pnl_amounts):.2f}
Worst Trade: ${min(pnl_amounts):.2f}"""
    ax4.text(0.05, 0.95, stats_text, transform=ax4.transAxes, fontsize=10,
        verticalalignment='top', bbox=dict(boxstyle='round', facecolor=
        'wheat', alpha=0.5))
    ax4.set_xlim(0, 1)
    ax4.set_ylim(0, 1)
    ax4.axis('off')
    ax4.set_title('Statistics Summary')
    plt.tight_layout()
    plt.show()

def save_performance_report(agent, game, filename='v4_performance_report.txt'):
    stats = game.get_trade_stats()
    portfolio_value = game.get_portfolio_value()
    total_return = (portfolio_value - game.initial_balance
        ) / game.initial_balance * 100
    report = f"""
V4 Forex DQN Performance Report
Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}
{'=' * 50}

TRAINING SUMMARY:
- Episodes Completed: {agent.n_games}
- Final Epsilon: {agent.epsilon}
- Memory Buffer Size: {len(agent.memory)}

PORTFOLIO PERFORMANCE:
- Initial Balance: ${game.initial_balance:,.2f}
- Final Portfolio Value: ${portfolio_value:,.2f}
- Total Return: {total_return:.2f}%
- Max Portfolio Value: ${game.max_balance:,.2f}
- Max Drawdown: {(game.max_balance - portfolio_value) / game.max_balance * 100:.2f}%

TRADING STATISTICS:
- Total Trades: {stats['total_trades']}
- Win Rate: {stats['win_rate']:.2%}
- Average P&L per Trade: ${stats['avg_pnl']:.2f}
- Total P&L: ${stats['total_pnl']:.2f}

RISK METRICS:
- Position Size: {game.position_size} lots (fixed)
- Risk per Trade: ~2% of account balance
- Maximum Single Loss: ${min([t['pnl_amount'] for t in game.trades_history]) if game.trades_history else 0:.2f}
- Maximum Single Gain: ${max([t['pnl_amount'] for t in game.trades_history]) if game.trades_history else 0:.2f}

MODEL ARCHITECTURE:
- Input Features: 4 (price_level, volatility, position_size, immediate_risk)
- Hidden Layer: 256 neurons
- Output Actions: 3 (Hold, Buy, Sell)
- Learning Rate: {LR}
- Discount Factor: {agent.gamma}

PHILOSOPHY VALIDATION:
‚úì Simplicity: Only 4 input features vs V3's complex indicators
‚úì Fixed Risk: Consistent 0.01 lot position sizing
‚úì Immediate Feedback: Binary profit/loss rewards
‚úì No Prediction: Pure timing-based approach
‚úì Risk Management: Built-in drawdown protection

{'=' * 50}================================================
File: model.py
================================================
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import numpy as np
import os
from config import V4Config, get_activation_function, create_model_config

class Linear_QNet(nn.Module):

    def __init__(self, input_size=13, output_size=3, config=None):
        super().__init__()
        if config is None:
            config = create_model_config('simple', 'fast')
        self.config = config
        network_config = config['network']
        self.layers = nn.ModuleList()
        self.dropouts = nn.ModuleList()
        self.batch_norms = nn.ModuleList()
        prev_size = input_size
        for hidden_size in network_config['hidden_layers']:
            self.layers.append(nn.Linear(prev_size, hidden_size))
            if network_config['batch_norm']:
                self.batch_norms.append(nn.BatchNorm1d(hidden_size))
            else:
                self.batch_norms.append(None)
            if network_config['dropout_rate'] > 0:
                self.dropouts.append(nn.Dropout(network_config['dropout_rate'])
                    )
            else:
                self.dropouts.append(None)
            prev_size = hidden_size
        self.output_layer = nn.Linear(prev_size, output_size)
        self.activation = get_activation_function(network_config['activation'])
        if config['advanced']['dueling_dqn']:
            self._setup_dueling_architecture(prev_size, output_size)
        self.device = self._setup_device(config)
        self.to(self.device)
        print(f'‚úì V4 DQN Model initialized:')
        print(
            f"  Architecture: {[input_size] + network_config['hidden_layers'] + [output_size]}"
            )
        print(f"  Activation: {network_config['activation']}")
        print(f"  Dropout: {network_config['dropout_rate']}")
        print(f"  Batch Norm: {network_config['batch_norm']}")
        print(f"  Dueling DQN: {config['advanced']['dueling_dqn']}")
        print(f'  Device: {self.device}')
        print(f'  Parameters: {sum(p.numel() for p in self.parameters()):,}')

    def _setup_device(self, config):
        if not config.get('gpu', {}).get('use_gpu', True
            ) or not torch.cuda.is_available():
            return torch.device('cpu')
        if torch.cuda.device_count() > 1:
            print(
                f'  Using {torch.cuda.device_count()} GPUs: {[torch.cuda.get_device_name(i) for i in range(torch.cuda.device_count())]}'
                )
            return torch.device('cuda')
        else:
            device_name = torch.cuda.get_device_name(0)
            print(f'  Using single GPU: {device_name}')
            return torch.device('cuda:0')

    def _setup_dueling_architecture(self, feature_size, action_size):
        self.value_stream = nn.Linear(feature_size, 1)
        self.advantage_stream = nn.Linear(feature_size, action_size)

    def forward(self, x):
        if not x.is_cuda and self.device.type == 'cuda':
            x = x.to(self.device)
        if len(x.shape) == 1:
            x = x.unsqueeze(0)
        for i, layer in enumerate(self.layers):
            x = layer(x)
            if self.batch_norms[i] is not None and x.size(0) > 1:
                x = self.batch_norms[i](x)
            x = self.activation(x)
            if self.dropouts[i] is not None:
                x = self.dropouts[i](x)
        if self.config['advanced']['dueling_dqn']:
            value = self.value_stream(x)
            advantage = self.advantage_stream(x)
            q_values = value + advantage - advantage.mean(dim=1, keepdim=True)
            return q_values
        else:
            return self.output_layer(x)

    def save(self, file_name='model.pth'):
        model_folder_path = './model'
        if not os.path.exists(model_folder_path):
            os.makedirs(model_folder_path)
        file_name = os.path.join(model_folder_path, file_name)
        torch.save({'model_state_dict': self.state_dict(), 'config': self.
            config}, file_name)
        print(f'‚úì Model saved: {file_name}')

    def load(self, file_name='model.pth'):
        model_folder_path = './model'
        file_path = os.path.join(model_folder_path, file_name)
        if os.path.exists(file_path):
            checkpoint = torch.load(file_path)
            self.load_state_dict(checkpoint['model_state_dict'])
            self.config = checkpoint.get('config', create_model_config(
                'simple', 'fast'))
            print(f'‚úì Model loaded: {file_path}')
            return True
        else:
            print(f'‚úó Model file not found: {file_path}')
            return False

class QTrainer:

    def __init__(self, model, config=None):
        if config is None:
            config = create_model_config('simple', 'fast')
        self.config = config
        training_config = config['training']
        self.lr = training_config['lr']
        self.gamma = training_config['gamma']
        self.model = model
        if config['advanced']['double_dqn']:
            self.target_model = Linear_QNet(input_size=config['state_size'],
                output_size=config['action_size'], config=config)
            self.target_model.load_state_dict(model.state_dict())
            self.target_update_freq = training_config['target_update_freq']
            self.update_count = 0
        else:
            self.target_model = None
        self.device = model.device
        self.optimizer = optim.Adam(model.parameters(), lr=self.lr)
        self.criterion = nn.MSELoss()
        self.gradient_clipping = config['advanced']['gradient_clipping']
        print(f'‚úì V4 QTrainer initialized:')
        print(f'  Learning Rate: {self.lr}')
        print(f'  Gamma: {self.gamma}')
        print(f"  Double DQN: {config['advanced']['double_dqn']}")
        print(f'  Gradient Clipping: {self.gradient_clipping}')

    def train_step(self, state, action, reward, next_state, done):
        if isinstance(state, (list, tuple)):
            state = torch.tensor(np.array(state), dtype=torch.float).to(self
                .device)
        else:
            state = torch.as_tensor(state, dtype=torch.float).to(self.device)
        if isinstance(next_state, (list, tuple)):
            next_state = torch.tensor(np.array(next_state), dtype=torch.float
                ).to(self.device)
        else:
            next_state = torch.as_tensor(next_state, dtype=torch.float).to(self
                .device)
        if isinstance(action, (list, tuple)):
            action = torch.tensor(np.array(action), dtype=torch.long).to(self
                .device)
        else:
            action = torch.as_tensor(action, dtype=torch.long).to(self.device)
        if isinstance(reward, (list, tuple)):
            reward = torch.tensor(np.array(reward), dtype=torch.float).to(self
                .device)
        else:
            reward = torch.as_tensor(reward, dtype=torch.float).to(self.device)
        if len(state.shape) == 1:
            state = torch.unsqueeze(state, 0)
            next_state = torch.unsqueeze(next_state, 0)
            action = torch.unsqueeze(action, 0)
            reward = torch.unsqueeze(reward, 0)
            done = done,
        pred = self.model(state)
        target = pred.clone()
        for idx in range(len(done)):
            Q_new = reward[idx]
            if not done[idx]:
                if self.target_model is not None:
                    if len(next_state.shape) == 1:
                        next_q_main = self.model(next_state)
                        best_action = torch.argmax(next_q_main).item()
                        next_q_target = self.target_model(next_state)
                        Q_new = reward[idx] + self.gamma * next_q_target[
                            best_action]
                    else:
                        next_q_main = self.model(next_state[idx].unsqueeze(0))
                        best_action = torch.argmax(next_q_main).item()
                        next_q_target = self.target_model(next_state[idx].
                            unsqueeze(0))
                        Q_new = reward[idx] + self.gamma * next_q_target[0][
                            best_action]
                elif len(next_state.shape) == 1:
                    Q_new = reward[idx] + self.gamma * torch.max(self.model
                        (next_state))
                else:
                    Q_new = reward[idx] + self.gamma * torch.max(self.model
                        (next_state[idx].unsqueeze(0)))
            if isinstance(action[idx], (int, torch.Tensor)) and action[idx
                ].dim() == 0:
                target[idx][action[idx]] = Q_new
            elif hasattr(action[idx], '__len__') and len(action[idx]) > 1:
                if torch.is_tensor(action[idx]):
                    action_idx = torch.argmax(action[idx].detach().clone()
                        ).item()
                else:
                    action_idx = torch.argmax(torch.as_tensor(action[idx])
                        ).item()
                target[idx][action_idx] = Q_new
            else:
                if isinstance(action[idx], list):
                    action_idx = action[idx].index(1) if 1 in action[idx
                        ] else 0
                elif torch.is_tensor(action[idx]):
                    action_idx = torch.argmax(action[idx].detach().clone()
                        ).item()
                else:
                    action_idx = torch.argmax(torch.as_tensor(action[idx])
                        ).item()
                target[idx][action_idx] = Q_new
        self.optimizer.zero_grad()
        loss = self.criterion(target, pred)
        loss.backward()
        if self.gradient_clipping > 0:
            torch.nn.utils.clip_grad_norm_(self.model.parameters(), self.
                gradient_clipping)
        self.optimizer.step()
        if self.target_model is not None:
            self.update_count += 1
            if self.update_count % self.target_update_freq == 0:
                self.target_model.load_state_dict(self.model.state_dict())
        return loss.item()

    def update_target_network(self):
        if self.target_model is not None:
            self.target_model.load_state_dict(self.model.state_dict())
            print('‚úì Target network updated')

def create_model_from_config(config_name='testing'):
    from config import PRESET_CONFIGS
    if config_name not in PRESET_CONFIGS:
        print(f"Warning: Unknown config '{config_name}', using 'testing'")
        config_name = 'testing'
    config = PRESET_CONFIGS[config_name]
    model = Linear_QNet(input_size=config['state_size'], output_size=config
        ['action_size'], config=config)
    trainer = QTrainer(model, config)
    print(f'‚úì Created {config_name.upper()} configuration')
    return model, trainer, config

================================================
File: parallel_grid_search.py
================================================
import torch
import torch.multiprocessing as mp
import pandas as pd
import numpy as np
import json
import time
from datetime import datetime
import os
import queue
from concurrent.futures import ThreadPoolExecutor, as_completed
from grid_search_config import V4GridSearchConfig
from agent import Agent
from forex_game import ForexGameAI

class ParallelGridSearchTrainer:

    def __init__(self, data_path=None, num_gpus=2):
        self.data_path = data_path
        self.num_gpus = num_gpus
        self.results = []
        self.data = None

    def load_data(self):
        if self.data is None:
            if self.data_path and os.path.exists(self.data_path):
                self.data = pd.read_csv(self.data_path)
                print(f'Loaded {len(self.data)} candles from {self.data_path}')
            else:
                from real_data_loader import load_real_gbpusd_data
                try:
                    self.data, _ = load_real_gbpusd_data()
                    print(f'Loaded real GBPUSD data: {len(self.data)} candles')
                except Exception as e:
                    raise ValueError(f'No data path provided and real data loading failed: {e}')
        return self.data

    def train_single_config(self, config, gpu_id, max_episodes=50):
        torch.cuda.set_device(gpu_id)
        device = torch.device(f'cuda:{gpu_id}')
        print(f"üöÄ GPU {gpu_id}: Starting {config['config_name']}")
        print(
            f"üîß GPU {gpu_id}: Layers {config['network']['hidden_layers']} | LR {config['training']['lr']}"
            )
        data = self.load_data()
        config['gpu']['device_ids'] = [gpu_id]
        agent = Agent()
        agent.model.to(device)
        agent.model.device = device
        agent.model.config = config
        agent.memory_size = config['training']['memory_size']
        agent.batch_size = config['training']['batch_size']
        agent.epsilon = config['training']['epsilon_start']
        agent.epsilon_decay_rate = config['training']['epsilon_decay']
        agent.epsilon_min = config['training']['epsilon_min']
        game = ForexGameAI(data)
        scores = []
        portfolio_values = []
        start_time = time.time()
        for episode in range(max_episodes):
            game.reset()
            episode_score = 0
            while True:
                state = agent.get_state(game)
                action = agent.get_action(state)
                reward, done, portfolio_value = game.play_step(action.index(1))
                next_state = agent.get_state(game)
                agent.train_short_memory(state, action, reward, next_state,
                    done)
                agent.remember(state, action, reward, next_state, done)
                episode_score += reward
                if done:
                    break
            agent.train_long_memory()
            scores.append(episode_score)
            portfolio_values.append(portfolio_value)
            if episode % 10 == 0:
                avg_score = np.mean(scores[-10:]) if len(scores
                    ) >= 10 else np.mean(scores)
                print(
                    f'üìä GPU {gpu_id}: Ep {episode:2d} | Score: {episode_score:6.1f} | Portfolio: ${portfolio_value:8.2f}'
                    )
        training_time = time.time() - start_time
        final_portfolio = portfolio_values[-1] if portfolio_values else 10000
        final_return = (final_portfolio - 10000) / 10000 * 100
        stats = game.get_trade_stats()
        result = {'config_id': config['config_id'], 'config_name': config[
            'config_name'], 'network': config['network'], 'training':
            config['training'], 'gpu_id': gpu_id, 'final_portfolio':
            final_portfolio, 'final_return': final_return, 'total_trades':
            stats.get('total_trades', 0), 'win_rate': stats.get('win_rate',
            0), 'avg_pnl': stats.get('avg_pnl', 0), 'total_pnl': stats.get(
            'total_pnl', 0), 'training_time': training_time,
            'episodes_completed': max_episodes}
        print(
            f"‚úÖ GPU {gpu_id}: {config['config_name']} | Return: {final_return:.2f}% | Trades: {stats.get('total_trades', 0)} | Time: {training_time:.1f}s"
            )
        return result

    def run_parallel_grid_search(self, max_configs=24, episodes_per_config=
        30, configs_per_gpu=6):
        print('üî• PARALLEL GRID SEARCH - MAXIMUM GPU UTILIZATION')
        print('=' * 80)
        print(
            f'üéØ Target: {self.num_gpus} GPUs √ó {configs_per_gpu} configs = {self.num_gpus * configs_per_gpu} parallel trainings'
            )
        self.load_data()
        grid_config = V4GridSearchConfig(max_neurons_per_layer=144)
        all_configs = grid_config.generate_search_space(max_configs=max_configs
            )
        results = []
        with ThreadPoolExecutor(max_workers=self.num_gpus * configs_per_gpu
            ) as executor:
            futures = []
            for i, config in enumerate(all_configs):
                gpu_id = i % self.num_gpus
                future = executor.submit(self.train_single_config, config,
                    gpu_id, episodes_per_config)
                futures.append(future)
                print(f"üì§ Submitted {config['config_name']} to GPU {gpu_id}")
                if len(futures) >= self.num_gpus * configs_per_gpu:
                    break
            print(f'\nüöÄ LAUNCHED {len(futures)} PARALLEL TRAININGS')
            print(
                f'‚ö° Expected GPU utilization: ~{len(futures) / self.num_gpus * 14:.0f}% per GPU'
                )
            for i, future in enumerate(as_completed(futures)):
                try:
                    result = future.result()
                    results.append(result)
                    print(
                        f"üèÅ Completed {i + 1}/{len(futures)}: {result['config_name']} | Return: {result['final_return']:.2f}%"
                        )
                except Exception as e:
                    print(f'‚ùå Training failed: {e}')
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        results_file = f'parallel_grid_search_results_{timestamp}.json'
        with open(results_file, 'w') as f:
            json.dump(results, f, indent=2)
        print(f"\n{'=' * 80}")
        print('üèÜ PARALLEL GRID SEARCH COMPLETED')
        print(f"{'=' * 80}")
        print(f'‚úÖ Completed {len(results)}/{len(futures)} configurations')
        print(f'üíæ Results saved to: {results_file}')
        if results:
            self.analyze_parallel_results(results)
            best_result = max(results, key=lambda x: x['final_return'])
            print(f'\nü•á BEST RESULT:')
            print(f"   Config: {best_result['config_name']}")
            print(f"   Layers: {best_result['network']['hidden_layers']}")
            print(f"   GPU: {best_result['gpu_id']}")
            print(f"   Return: {best_result['final_return']:.2f}%")
            print(f"   Time: {best_result['training_time']:.1f}s")

    def analyze_parallel_results(self, results):
        if not results:
            return
        print(f'\nüìà PERFORMANCE ANALYSIS:')
        sorted_results = sorted(results, key=lambda x: x['final_return'],
            reverse=True)
        print(f'\nüèÜ TOP 5 PERFORMERS:')
        for i, result in enumerate(sorted_results[:5]):
            print(f"   {i + 1}. {result['config_name']}")
            print(f"      Layers: {result['network']['hidden_layers']}")
            print(
                f"      Return: {result['final_return']:.2f}% | GPU: {result['gpu_id']} | Time: {result['training_time']:.1f}s"
                )
        gpu_performance = {}
        for result in results:
            gpu = result['gpu_id']
            if gpu not in gpu_performance:
                gpu_performance[gpu] = []
            gpu_performance[gpu].append(result['final_return'])
        print(f'\nüéÆ GPU PERFORMANCE:')
        for gpu_id, returns in gpu_performance.items():
            avg_return = np.mean(returns)
            max_return = max(returns)
            count = len(returns)
            print(
                f'   GPU {gpu_id}: {count} configs | Avg: {avg_return:.2f}% | Best: {max_return:.2f}%'
                )
        layer_performance = {}
        for result in results:
            layers = tuple(result['network']['hidden_layers'])
            if layers not in layer_performance:
                layer_performance[layers] = []
            layer_performance[layers].append(result['final_return'])
        avg_layer_performance = {layers: np.mean(returns) for layers,
            returns in layer_performance.items()}
        best_layers = sorted(avg_layer_performance.items(), key=lambda x: x
            [1], reverse=True)
        print(f'\nüß† BEST LAYER ARCHITECTURES:')
        for i, (layers, avg_return) in enumerate(best_layers[:5]):
            param_count = self.calculate_parameters(list(layers))
            print(
                f'   {i + 1}. {list(layers)} | Avg Return: {avg_return:.2f}% | Params: {param_count:,}'
                )

    def calculate_parameters(self, hidden_layers):
        input_size = 8
        output_size = 3
        total_params = 0
        prev_size = input_size
        for layer_size in hidden_layers:
            total_params += prev_size * layer_size + layer_size
            prev_size = layer_size
        total_params += prev_size * output_size + output_size
        return total_params

def main():
    mp.set_start_method('spawn', force=True)
    real_data_path = '/home/hung/Public/Test/FX/data/GBPUSD60.csv'
    trainer = ParallelGridSearchTrainer(real_data_path, num_gpus=2)
    trainer.run_parallel_grid_search(max_configs=24, episodes_per_config=25,
        configs_per_gpu=6)

if __name__ == '__main__':
    main()

================================================
File: real_data_loader.py
================================================
import pandas as pd
import numpy as np
from datetime import datetime

def load_real_gbpusd_data(h1_path=
    '/home/hung/Public/Test/FX/data/GBPUSD60.csv', d1_path=
    '/home/hung/Public/Test/FX/data/GBPUSD1440.csv', start_samples=1000,
    total_samples=None):
    print(f'Loading real GBPUSD data...')
    print(f'H1 path: {h1_path}')
    print(f'D1 path: {d1_path}')
    h1_data = pd.read_csv(h1_path)
    d1_data = pd.read_csv(d1_path)
    print(f'H1 data loaded: {len(h1_data):,} records')
    print(f'D1 data loaded: {len(d1_data):,} records')
    h1_data['time'] = pd.to_datetime(h1_data['time'])
    d1_data['time'] = pd.to_datetime(d1_data['time'])
    h1_data = h1_data.sort_values('time').reset_index(drop=True)
    d1_data = d1_data.sort_values('time').reset_index(drop=True)
    if total_samples is None:
        total_samples = len(h1_data) - start_samples
        print(f'Using full dataset: {total_samples:,} samples')
    elif len(h1_data) < start_samples + total_samples:
        print(
            f'Warning: Not enough H1 data. Available: {len(h1_data)}, Requested: {start_samples + total_samples}'
            )
        total_samples = len(h1_data) - start_samples
    
    h1_filtered = h1_data.copy()
    h1_filtered = h1_filtered.dropna()
    
    if 'volume' in h1_filtered.columns:
        volume_threshold = h1_filtered['volume'].quantile(0.1)
        h1_filtered = h1_filtered[h1_filtered['volume'] > volume_threshold]
    
    h1_filtered['hour'] = h1_filtered['time'].dt.hour
    weekend_mask = ~((h1_filtered['time'].dt.dayofweek >= 5) & (h1_filtered['hour'] >= 22))
    h1_filtered = h1_filtered[weekend_mask]
    
    h1_subset = h1_filtered.iloc[start_samples:start_samples + total_samples].copy()
    h1_subset = h1_subset.reset_index(drop=True)
    print(
        f"Using H1 data from {h1_subset['time'].iloc[0]} to {h1_subset['time'].iloc[-1]}"
        )
    print(
        f"Price range: {h1_subset['close'].min():.5f} - {h1_subset['close'].max():.5f}"
        )
    print(
        f"Total price movement: {(h1_subset['close'].iloc[-1] / h1_subset['close'].iloc[0] - 1) * 100:.2f}%"
        )
    return h1_subset, d1_data

def get_data_statistics(data):
    stats = {'total_records': len(data), 'date_range':
        f"{data['time'].iloc[0]} to {data['time'].iloc[-1]}", 'price_min':
        data['close'].min(), 'price_max': data['close'].max(), 'price_mean':
        data['close'].mean(), 'price_std': data['close'].std(),
        'total_return_pct': (data['close'].iloc[-1] / data['close'].iloc[0] -
        1) * 100, 'avg_volume': data['volume'].mean() if 'volume' in data.
        columns else 0}
    return stats

if __name__ == '__main__':
    h1_data, d1_data = load_real_gbpusd_data()
    print('\nH1 Data Statistics:')
    h1_stats = get_data_statistics(h1_data)
    for key, value in h1_stats.items():
        print(f'  {key}: {value}')
    print('\nD1 Data Statistics:')
    d1_stats = get_data_statistics(d1_data)
    for key, value in d1_stats.items():
        print(f'  {key}: {value}')

================================================
File: simple_risk_manager.py
================================================
import numpy as np
from datetime import datetime, timedelta
from typing import Dict, List, Tuple
from dataclasses import dataclass

@dataclass
class V4RiskMetrics:
    account_balance: float
    current_drawdown: float
    daily_pnl: float
    total_trades: int
    win_rate: float
    risk_violations: List[str]
    stop_trading: bool

class V4SimpleRiskManager:

    def __init__(self, initial_balance: float=10000):
        self.initial_balance = initial_balance
        self.current_balance = initial_balance
        self.max_balance = initial_balance
        self.MAX_DRAWDOWN = 0.2
        self.DAILY_LOSS_LIMIT = 0.05
        self.FIXED_POSITION_SIZE = 0.01
        self.MAX_CONSECUTIVE_LOSSES = 5
        self.daily_pnl = 0.0
        self.trades_today = 0
        self.trade_history = []
        self.consecutive_losses = 0
        self.last_reset_date = datetime.now().date()
        self.violations = []
        print(f'‚úì V4 Risk Manager initialized')
        print(f'  Fixed position size: {self.FIXED_POSITION_SIZE} lots')
        print(f'  Max drawdown: {self.MAX_DRAWDOWN:.1%}')
        print(f'  Daily loss limit: {self.DAILY_LOSS_LIMIT:.1%}')

    def reset_daily_tracking(self):
        current_date = datetime.now().date()
        if current_date > self.last_reset_date:
            self.daily_pnl = 0.0
            self.trades_today = 0
            self.last_reset_date = current_date

    def validate_trade(self, symbol: str, direction: str, current_price:
        float=0) ->Tuple[bool, List[str]]:
        self.reset_daily_tracking()
        violations = []
        current_drawdown = (self.max_balance - self.current_balance
            ) / self.max_balance
        if current_drawdown >= self.MAX_DRAWDOWN:
            violations.append(
                f'Maximum drawdown exceeded: {current_drawdown:.1%}')
        daily_loss_pct = abs(self.daily_pnl
            ) / self.current_balance if self.daily_pnl < 0 else 0
        if daily_loss_pct >= self.DAILY_LOSS_LIMIT:
            violations.append(
                f'Daily loss limit exceeded: {daily_loss_pct:.1%}')
        if self.consecutive_losses >= self.MAX_CONSECUTIVE_LOSSES:
            violations.append(
                f'Too many consecutive losses: {self.consecutive_losses}')
        if self.current_balance < self.initial_balance * 0.5:
            violations.append('Account balance below 50% of initial capital')
        current_time = datetime.now()
        if current_time.weekday() >= 5:
            violations.append('Weekend trading not allowed')
        is_valid = len(violations) == 0
        if violations:
            self.violations.extend(violations)
        return is_valid, violations

    def record_trade_result(self, pnl: float, trade_details: Dict=None):
        self.reset_daily_tracking()
        self.current_balance += pnl
        self.daily_pnl += pnl
        self.trades_today += 1
        if self.current_balance > self.max_balance:
            self.max_balance = self.current_balance
        if pnl < 0:
            self.consecutive_losses += 1
        else:
            self.consecutive_losses = 0
        trade_record = {'timestamp': datetime.now(), 'pnl': pnl, 'balance':
            self.current_balance, 'daily_pnl': self.daily_pnl,
            'consecutive_losses': self.consecutive_losses, 'drawdown': (
            self.max_balance - self.current_balance) / self.max_balance}
        if trade_details:
            trade_record.update(trade_details)
        self.trade_history.append(trade_record)
        if pnl < 0 and self.consecutive_losses >= 3:
            print(f'‚ö†Ô∏è  {self.consecutive_losses} consecutive losses')
        if self.current_balance > self.max_balance * 0.95:
            print(f'‚úì Near peak balance: ${self.current_balance:.2f}')

    def get_risk_metrics(self) ->V4RiskMetrics:
        self.reset_daily_tracking()
        current_drawdown = (self.max_balance - self.current_balance
            ) / self.max_balance
        if self.trade_history:
            winning_trades = [t for t in self.trade_history if t['pnl'] > 0]
            win_rate = len(winning_trades) / len(self.trade_history)
        else:
            win_rate = 0.0
        active_violations = []
        if current_drawdown >= self.MAX_DRAWDOWN:
            active_violations.append('Maximum drawdown exceeded')
        daily_loss_pct = abs(self.daily_pnl
            ) / self.current_balance if self.daily_pnl < 0 else 0
        if daily_loss_pct >= self.DAILY_LOSS_LIMIT:
            active_violations.append('Daily loss limit exceeded')
        if self.consecutive_losses >= self.MAX_CONSECUTIVE_LOSSES:
            active_violations.append('Too many consecutive losses')
        stop_trading = len(active_violations
            ) > 0 or self.current_balance < self.initial_balance * 0.5
        return V4RiskMetrics(account_balance=self.current_balance,
            current_drawdown=current_drawdown, daily_pnl=daily_loss_pct,
            total_trades=len(self.trade_history), win_rate=win_rate,
            risk_violations=active_violations, stop_trading=stop_trading)

    def should_stop_trading(self) ->bool:
        metrics = self.get_risk_metrics()
        return metrics.stop_trading

    def get_position_size(self) ->float:
        return self.FIXED_POSITION_SIZE

    def get_trading_summary(self) ->Dict:
        if not self.trade_history:
            return {'message': 'No trades recorded', 'initial_balance':
                self.initial_balance, 'current_balance': self.current_balance}
        total_pnl = sum(t['pnl'] for t in self.trade_history)
        total_return = total_pnl / self.initial_balance
        profitable_trades = [t for t in self.trade_history if t['pnl'] > 0]
        losing_trades = [t for t in self.trade_history if t['pnl'] < 0]
        win_rate = len(profitable_trades) / len(self.trade_history)
        avg_win = np.mean([t['pnl'] for t in profitable_trades]
            ) if profitable_trades else 0
        avg_loss = np.mean([t['pnl'] for t in losing_trades]
            ) if losing_trades else 0
        profit_factor = abs(avg_win / avg_loss) if avg_loss != 0 else 0
        max_drawdown = max([t['drawdown'] for t in self.trade_history])
        return {'initial_balance': self.initial_balance, 'current_balance':
            self.current_balance, 'total_pnl': total_pnl, 'total_return':
            total_return, 'total_trades': len(self.trade_history),
            'winning_trades': len(profitable_trades), 'losing_trades': len(
            losing_trades), 'win_rate': win_rate, 'avg_win': avg_win,
            'avg_loss': avg_loss, 'profit_factor': profit_factor,
            'max_drawdown': max_drawdown, 'consecutive_losses': self.
            consecutive_losses, 'daily_pnl': self.daily_pnl,
            'fixed_position_size': self.FIXED_POSITION_SIZE,
            'risk_violations': len(self.violations)}

    def reset_account(self, new_balance: float=None):
        if new_balance:
            self.initial_balance = new_balance
            self.current_balance = new_balance
            self.max_balance = new_balance
        else:
            self.current_balance = self.initial_balance
            self.max_balance = self.initial_balance
        self.daily_pnl = 0.0
        self.trades_today = 0
        self.consecutive_losses = 0
        self.trade_history = []
        self.violations = []
        self.last_reset_date = datetime.now().date()
        print(f'‚úì Account reset to ${self.current_balance:,.2f}')

def main():
    print('V4 Simple Risk Manager Demo')
    print('=' * 40)
    risk_mgr = V4SimpleRiskManager(10000)
    print('\nSimulating trades:')
    trade_scenarios = [(150, 'Small win'), (-80, 'Small loss'), (300,
        'Good win'), (-120, 'Loss'), (-150, 'Another loss'), (-100,
        'Third loss'), (200, 'Recovery win'), (-200, 'Larger loss'), (-180,
        'Another loss - should trigger consecutive loss warning')]
    for pnl, description in trade_scenarios:
        is_valid, violations = risk_mgr.validate_trade('EURUSD', 'buy', 1.2)
        if is_valid:
            risk_mgr.record_trade_result(pnl, {'description': description})
            print(
                f'‚úì {description}: ${pnl:+.2f} | Balance: ${risk_mgr.current_balance:.2f}'
                )
        else:
            print(f'‚úó Trade blocked: {violations}')
            break
        if risk_mgr.should_stop_trading():
            print('üõë Trading stopped due to risk limits!')
            break
    print('\nFinal Risk Assessment:')
    metrics = risk_mgr.get_risk_metrics()
    print(f'  Account Balance: ${metrics.account_balance:.2f}')
    print(f'  Current Drawdown: {metrics.current_drawdown:.1%}')
    print(f'  Daily P&L: {risk_mgr.daily_pnl:+.2f}')
    print(f'  Total Trades: {metrics.total_trades}')
    print(f'  Win Rate: {metrics.win_rate:.1%}')
    print(f'  Should Stop Trading: {metrics.stop_trading}')
    if metrics.risk_violations:
        print(f'  Violations: {metrics.risk_violations}')
    print('\nTrading Summary:')
    summary = risk_mgr.get_trading_summary()
    for key, value in summary.items():
        if isinstance(value, float):
            if 'rate' in key or 'return' in key:
                print(f'  {key}: {value:.1%}')
            else:
                print(f'  {key}: {value:.2f}')
        else:
            print(f'  {key}: {value}')

if __name__ == '__main__':
    main()

================================================
File: test_config.py
================================================
import sys
import os
sys.path.append(os.path.dirname(__file__))
from config import PRESET_CONFIGS, print_config_summary, create_model_config
from model import create_model_from_config

def test_all_configurations():
    print('üß™ Testing V4 Configurable DQN Architecture')
    print('=' * 60)
    for config_name in PRESET_CONFIGS.keys():
        print(f'\nüîß Testing {config_name.upper()} Configuration')
        print('-' * 40)
        try:
            model, trainer, config = create_model_from_config(config_name)
            import torch
            test_input = torch.randn(1, 4)
            with torch.no_grad():
                output = model(test_input)
            print(f'‚úÖ Model created successfully')
            print(f'   Input shape: {test_input.shape}')
            print(f'   Output shape: {output.shape}')
            print(
                f"   Network layers: {len(config['network']['hidden_layers'])}"
                )
            print(
                f'   Parameters: {sum(p.numel() for p in model.parameters()):,}'
                )
        except Exception as e:
            print(f'‚ùå Error with {config_name}: {e}')
    print(f'\n‚úÖ All configuration tests completed!')

def test_snake_vs_sophisticated():
    print('\nüêç Snake vs Sophisticated Architecture Comparison')
    print('=' * 60)
    configs_to_compare = ['development', 'testing', 'competition']
    for config_name in configs_to_compare:
        model, trainer, config = create_model_from_config(config_name)
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.
            requires_grad)
        print(f'\n{config_name.upper()}:')
        print(
            f"  Architecture: {[4] + config['network']['hidden_layers'] + [3]}"
            )
        print(f'  Total Parameters: {total_params:,}')
        print(f'  Trainable Parameters: {trainable_params:,}')
        print(f"  Memory Size: {config['training']['memory_size']:,}")
        print(f"  Batch Size: {config['training']['batch_size']:,}")
        print(f"  Learning Rate: {config['training']['lr']}")
        print(f'  Advanced Features:')
        print(f"    ‚Ä¢ Double DQN: {config['advanced']['double_dqn']}")
        print(f"    ‚Ä¢ Dueling DQN: {config['advanced']['dueling_dqn']}")
        print(
            f"    ‚Ä¢ Prioritized Replay: {config['advanced']['prioritized_replay']}"
            )

def show_config_details():
    print('\nüìã Configuration Details')
    print('=' * 60)
    print_config_summary('testing')
    print(f'\nüéØ V4 Philosophy Maintained:')
    print(
        f'  ‚úì State Space: Always 4 inputs (price_level, volatility, position_size, immediate_risk)'
        )
    print(f'  ‚úì Action Space: Always 3 outputs (Hold, Buy, Sell)')
    print(f'  ‚úì Simplicity: No technical indicators, just raw market state')
    print(
        f'  ‚úì Configurable: Network complexity adjustable while keeping V4 principles'
        )

def main():
    print('V4 Configurable DQN Architecture Test Suite')
    print('=' * 80)
    test_all_configurations()
    test_snake_vs_sophisticated()
    show_config_details()
    print(f'\nüéâ Test suite completed successfully!')
    print(
        f"üí° You can now use sophisticated DQN architectures while maintaining V4's simple 4-input philosophy!"
        )
    print(f'\nExample usage:')
    print(
        f'  python agent.py --config development --episodes 50   # Fast testing'
        )
    print(
        f'  python agent.py --config testing --episodes 200     # Standard training'
        )
    print(
        f'  python agent.py --config competition --episodes 1000 # Full sophistication'
        )

if __name__ == '__main__':
    main()

================================================
File: trainer.py
================================================
import torch
import pandas as pd
import numpy as np
from datetime import datetime
import os
import sys
sys.path.append(os.path.dirname(os.path.abspath(__file__)))
from agent import Agent
from forex_game import ForexGameAI
from helper import plot, plot_trade_analysis, save_performance_report

class V4Trainer:

    def __init__(self, data_path=None):
        self.data_path = data_path
        self.agent = Agent()
        self.game = None
        self.scores = []
        self.mean_scores = []
        self.portfolio_values = []
        self.total_score = 0
        self.record = 0
        self.best_portfolio = 10000

    def load_data(self):
        if self.data_path and os.path.exists(self.data_path):
            try:
                df = pd.read_csv(self.data_path)
                print(
                    f'Loaded real data: {len(df)} candles from {self.data_path}'
                    )
                return df
            except Exception as e:
                print(f'Error loading data: {e}')
                print('Falling back to synthetic data...')
        from real_data_loader import load_real_gbpusd_data
        try:
            data, _ = load_real_gbpusd_data()
            print(f'Loaded real GBPUSD data: {len(data)} candles')
            return data
        except Exception as e:
            print(f'Error loading real data: {e}')
            raise RuntimeError('No data available for training')

    def train(self, max_episodes=1000, save_interval=100, plot_interval=10):
        print('Starting V4 Forex DQN Training')
        print('Philosophy: Simplicity beats complexity')
        print('State: 4 simple inputs, Actions: 3 simple outputs')
        print('=' * 60)
        data = self.load_data()
        self.game = ForexGameAI(data)
        while self.agent.n_games < max_episodes:
            state_old = self.agent.get_state(self.game)
            final_move = self.agent.get_action(state_old)
            action_name = ['HOLD', 'BUY', 'SELL'][final_move.index(1)]
            reward, done, portfolio_value = self.game.play_step(final_move.
                index(1))
            state_new = self.agent.get_state(self.game)
            if self.game.current_step % 100 == 0:
                current_price = self.game.data.iloc[self.game.current_step][
                    'close'] if self.game.current_step < len(self.game.data
                    ) else 0
                print(
                    f'Step {self.game.current_step:5d} | Price: ${current_price:.5f} | Action: {action_name} | Reward: {reward:5.1f} | Portfolio: ${portfolio_value:.2f}'
                    )
            self.agent.train_short_memory(state_old, final_move, reward,
                state_new, done)
            self.agent.remember(state_old, final_move, reward, state_new, done)
            if done:
                self.game.reset()
                self.agent.n_games += 1
                self.agent.train_long_memory()
                total_return = (portfolio_value - self.game.initial_balance
                    ) / self.game.initial_balance * 100
                if total_return > self.record:
                    self.record = total_return
                    self.agent.model.save('best_return_model.pth')
                if portfolio_value > self.best_portfolio:
                    self.best_portfolio = portfolio_value
                    self.agent.model.save(
                        f'best_portfolio_${int(portfolio_value)}.pth')
                self.scores.append(total_return)
                self.total_score += total_return
                mean_score = self.total_score / self.agent.n_games
                self.mean_scores.append(mean_score)
                self.portfolio_values.append(portfolio_value)
                print(
                    f'Episode {self.agent.n_games:4d} | Portfolio: ${portfolio_value:8.2f} | Return: {total_return:6.2f}% | Record: {self.record:6.2f}% | Œµ: {max(0, self.agent.epsilon):3.0f}'
                    )
                stats = self.game.get_trade_stats()
                if stats['total_trades'] > 0:
                    print(
                        f"         Trades: {stats['total_trades']:3d} | Win Rate: {stats['win_rate']:5.1%} | Avg P&L: ${stats['avg_pnl']:6.2f} | Total P&L: ${stats['total_pnl']:7.2f}"
                        )
                if self.agent.n_games % plot_interval == 0:
                    try:
                        plot(self.scores, self.mean_scores, self.
                            portfolio_values)
                    except Exception as e:
                        print(f'Plotting error: {e}')
                if self.agent.n_games % save_interval == 0:
                    self.save_checkpoint()
                    print(f'Checkpoint saved at episode {self.agent.n_games}')
        print(f'\nTraining completed after {self.agent.n_games} episodes!')
        self.save_final_results()

    def save_checkpoint(self):
        checkpoint = {'episode': self.agent.n_games, 'model_state_dict':
            self.agent.model.state_dict(), 'scores': self.scores,
            'mean_scores': self.mean_scores, 'portfolio_values': self.
            portfolio_values, 'record': self.record, 'best_portfolio': self
            .best_portfolio}
        torch.save(checkpoint, f'checkpoint_episode_{self.agent.n_games}.pth')

    def load_checkpoint(self, checkpoint_path):
        if os.path.exists(checkpoint_path):
            checkpoint = torch.load(checkpoint_path)
            self.agent.model.load_state_dict(checkpoint['model_state_dict'])
            self.agent.n_games = checkpoint['episode']
            self.scores = checkpoint['scores']
            self.mean_scores = checkpoint['mean_scores']
            self.portfolio_values = checkpoint['portfolio_values']
            self.record = checkpoint['record']
            self.best_portfolio = checkpoint['best_portfolio']
            print(f'Loaded checkpoint from episode {self.agent.n_games}')
            return True
        return False

    def save_final_results(self):
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        self.agent.model.save(f'final_model_{timestamp}.pth')
        save_performance_report(self.agent, self.game,
            f'performance_report_{timestamp}.txt')
        try:
            plot_trade_analysis(self.game.trades_history)
            print('Trade analysis plot displayed')
        except Exception as e:
            print(f'Could not generate trade analysis plot: {e}')
        final_portfolio = self.portfolio_values[-1
            ] if self.portfolio_values else self.game.initial_balance
        final_return = (final_portfolio - self.game.initial_balance
            ) / self.game.initial_balance * 100
        print(f"\n{'=' * 60}")
        print('FINAL TRAINING SUMMARY')
        print(f"{'=' * 60}")
        print(f'Episodes: {self.agent.n_games}')
        print(f'Final Portfolio: ${final_portfolio:,.2f}')
        print(f'Total Return: {final_return:.2f}%')
        print(f'Best Portfolio: ${self.best_portfolio:,.2f}')
        print(f'Best Return: {self.record:.2f}%')
        if self.game.trades_history:
            stats = self.game.get_trade_stats()
            print(f"Total Trades: {stats['total_trades']}")
            print(f"Win Rate: {stats['win_rate']:.2%}")
            print(f"Avg P&L: ${stats['avg_pnl']:.2f}")
        print(f'\nV4 Philosophy Validation:')
        print(f'‚úì Simple State: 4 inputs only')
        print(f'‚úì Fixed Risk: 0.01 lots per trade')
        print(f'‚úì No Indicators: Pure price/volatility/risk')
        print(f'‚úì Immediate Feedback: Binary profit/loss rewards')
        print(f'‚úì Continuous Learning: {len(self.agent.memory)} experiences')

def main():
    real_data_path = '/home/hung/Public/Test/FX/data/GBPUSD60.csv'
    trainer = V4Trainer(real_data_path)
    latest_checkpoint = None
    for file in os.listdir('.'):
        if file.startswith('checkpoint_episode_') and file.endswith('.pth'):
            latest_checkpoint = file
    if latest_checkpoint:
        response = input(
            f'Found checkpoint {latest_checkpoint}. Resume training? (y/n): ')
        if response.lower() == 'y':
            trainer.load_checkpoint(latest_checkpoint)
    try:
        trainer.train(max_episodes=500, save_interval=50, plot_interval=10)
    except KeyboardInterrupt:
        print('\nTraining interrupted by user')
        trainer.save_checkpoint()
        trainer.save_final_results()
    except Exception as e:
        print(f'Training error: {e}')
        trainer.save_checkpoint()

if __name__ == '__main__':
    main()

