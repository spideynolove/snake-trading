Directory structure:
└── snake-trading/
    ├── run.py
    ├── core/
    │   ├── agent.py
    │   ├── config.py
    │   ├── model.py
    │   └── replay_buffer.py
    ├── game_env/
    │   ├── base_env.py
    │   ├── forex_env.py
    │   ├── gym_adapter.py
    │   └── hierarchical_env.py
    ├── integration/
    │   ├── data_feed.py
    │   ├── dwx_connector.py
    │   └── signal_translator.py
    ├── materials/
    │   └── design/
    ├── notebooks/
    ├── tests/
    │   ├── test_agent.py
    │   ├── test_env.py
    │   └── test_trainer.py
    ├── training/
    │   ├── evaluator.py
    │   └── trainer.py
    └── utils/
        ├── logger.py
        ├── metrics.py
        ├── plots.py
        └── risk.py

================================================
File: run.py
================================================
import pandas as pd
import sys
from pathlib import Path
from core.agent import Agent
from env.forex_env import ForexEnv
from integration.data_feed import SequentialProcessor, DataFeedThread

def train(csv_path=None, use_sequential=True):
    if csv_path is None:
        print("Error: CSV file path required")
        print("Usage: python run.py --csv path/to/data.csv")
        return
    
    try:
        data_feed = DataFeedThread(csv_path=csv_path)
        data = data_feed.data
        print(f"Loaded {len(data)} data points from {csv_path}")
    except Exception as e:
        print(f"Error loading data: {e}")
        return
    
    plot_scores = []
    plot_mean_scores = []
    total_score = 0
    record = 0
    agent = Agent()
    game = ForexEnv(data)
    
    print("Starting Snake-inspired Forex Trading AI training...")
    print(f"Data points: {len(data)}")
    print(f"Model: 4 inputs -> 256 hidden -> 3 outputs")
    print(f"Actions: 0=Close, 1=Long, 2=Short")
    print(f"Rewards: +10 profit, -10 loss")
    print(f"Processing mode: {'Sequential' if use_sequential else 'Threaded'}")
    
    if use_sequential:
        sequential_processor = SequentialProcessor(agent, game)
    
    while True:
        if use_sequential:
            try:
                final_balance = sequential_processor.process_episode(data)
                agent.n_games += 1
                agent.train_long_memory()
                
                if final_balance > record:
                    record = final_balance
                    agent.model.save()
                
                print(f'Game {agent.n_games}, Final Balance: ${final_balance:.2f}, Record: ${record:.2f}, Trades: {len(game.trades_history)}')
                
                plot_scores.append(final_balance)
                total_score += final_balance
                mean_score = total_score / agent.n_games
                plot_mean_scores.append(mean_score)
                
                game.reset()
                
            except Exception as e:
                print(f"Episode failed: {e}")
                break
        else:
            state_old = agent.get_state(game)
            final_move = agent.get_action(state_old)
            
            action = final_move.index(1)
            reward, done, score = game.step(action)
            state_new = agent.get_state(game)
            
            agent.train_short_memory(state_old, final_move, reward, state_new, done)
            agent.remember(state_old, final_move, reward, state_new, done)
            
            if done:
                game.reset()
                agent.n_games += 1
                agent.train_long_memory()
                
                final_balance = score
                if final_balance > record:
                    record = final_balance
                    agent.model.save()
                
                print(f'Game {agent.n_games}, Final Balance: ${final_balance:.2f}, Record: ${record:.2f}, Trades: {len(game.trades_history)}')
                
                plot_scores.append(final_balance)
                total_score += final_balance
                mean_score = total_score / agent.n_games
                plot_mean_scores.append(mean_score)

if __name__ == '__main__':
    import argparse
    
    parser = argparse.ArgumentParser(description='Snake-inspired Forex Trading AI')
    parser.add_argument('--csv', type=str, required=True, help='Path to GBPUSD H1 CSV file')
    parser.add_argument('--mode', choices=['sequential', 'threaded'], default='sequential', help='Processing mode')
    
    args = parser.parse_args()
    
    use_sequential = args.mode == 'sequential'
    train(csv_path=args.csv, use_sequential=use_sequential)

================================================
File: core/agent.py
================================================
import torch
import random
import numpy as np
from collections import deque
import threading
from core.model import Linear_QNet, QTrainer

MAX_MEMORY = 100_000
BATCH_SIZE = 1000
LR = 0.001

class Agent:
    def __init__(self):
        self.n_games = 0
        self.epsilon = 0
        self.gamma = 0.9
        self.memory = deque(maxlen=MAX_MEMORY)
        self.model = Linear_QNet(4, 256, 3)
        self.trainer = QTrainer(self.model, lr=LR, gamma=self.gamma)
        self.decision_lock = threading.Lock()

    def get_state(self, game):
        return game.get_state()

    def remember(self, state, action, reward, next_state, done):
        self.memory.append((state, action, reward, next_state, done))

    def train_long_memory(self):
        if len(self.memory) > BATCH_SIZE:
            mini_sample = random.sample(self.memory, BATCH_SIZE)
        else:
            mini_sample = self.memory

        states, actions, rewards, next_states, dones = zip(*mini_sample)
        self.trainer.train_step(states, actions, rewards, next_states, dones)

    def train_short_memory(self, state, action, reward, next_state, done):
        self.trainer.train_step(state, action, reward, next_state, done)

    def get_action(self, state):
        with self.decision_lock:
            self.epsilon = 80 - self.n_games
            final_move = [0, 0, 0]
            if random.randint(0, 200) < self.epsilon:
                move = random.randint(0, 2)
                final_move[move] = 1
            else:
                state0 = torch.tensor(state, dtype=torch.float)
                prediction = self.model(state0)
                move = torch.argmax(prediction).item()
                final_move[move] = 1

            return final_move

================================================
File: core/config.py
================================================
MAX_MEMORY = 100_000
BATCH_SIZE = 1000
LR = 0.001

MODEL_INPUT_SIZE = 4
MODEL_HIDDEN_SIZE = 256
MODEL_OUTPUT_SIZE = 3

INITIAL_BALANCE = 10000.0
GAMMA = 0.9

EPSILON_START = 80
EPSILON_DECAY_RATE = 1
EPSILON_MIN = 0

ACTION_CLOSE = 0
ACTION_LONG = 1  
ACTION_SHORT = 2

REWARD_PROFIT = 10
REWARD_LOSS = -10

REQUIRED_CSV_COLUMNS = ['timestamp', 'open', 'high', 'low', 'close', 'volume']

MARKET_GAP_THRESHOLD_HOURS = 2

FEATURE_MOMENTUM_LOOKBACK = 5
FEATURE_MOMENTUM_CLIP = 0.05
FEATURE_PNL_CLIP = 0.2

LOG_INTERVAL = 10
CHECKPOINT_INTERVAL = 100

PLOT_DPI = 300
PLOT_FIGSIZE_EQUITY = (12, 6)
PLOT_FIGSIZE_ANALYSIS = (15, 10)

================================================
File: core/model.py
================================================
import torch
import torch.nn as nn
import torch.optim as optim
import torch.nn.functional as F
import os

class Linear_QNet(nn.Module):
    def __init__(self, input_size=4, hidden_size=256, output_size=3):
        super().__init__()
        self.linear1 = nn.Linear(input_size, hidden_size)
        self.linear2 = nn.Linear(hidden_size, output_size)

    def forward(self, x):
        x = F.relu(self.linear1(x))
        x = self.linear2(x)
        return x

    def save(self, file_name='model.pth'):
        model_folder_path = './model'
        if not os.path.exists(model_folder_path):
            os.makedirs(model_folder_path)

        file_name = os.path.join(model_folder_path, file_name)
        torch.save(self.state_dict(), file_name)

    def load(self, file_name='model.pth'):
        model_folder_path = './model'
        file_name = os.path.join(model_folder_path, file_name)
        if os.path.exists(file_name):
            self.load_state_dict(torch.load(file_name))

class QTrainer:
    def __init__(self, model, lr=0.001, gamma=0.9):
        self.lr = lr
        self.gamma = gamma
        self.model = model
        self.optimizer = optim.Adam(model.parameters(), lr=self.lr)
        self.criterion = nn.MSELoss()

    def train_step(self, state, action, reward, next_state, done):
        state = torch.tensor(state, dtype=torch.float)
        next_state = torch.tensor(next_state, dtype=torch.float)
        action = torch.tensor(action, dtype=torch.long)
        reward = torch.tensor(reward, dtype=torch.float)

        if len(state.shape) == 1:
            state = torch.unsqueeze(state, 0)
            next_state = torch.unsqueeze(next_state, 0)
            action = torch.unsqueeze(action, 0)
            reward = torch.unsqueeze(reward, 0)
            done = (done, )

        pred = self.model(state)

        target = pred.clone()
        for idx in range(len(done)):
            Q_new = reward[idx]
            if not done[idx]:
                Q_new = reward[idx] + self.gamma * torch.max(self.model(next_state[idx]))

            target[idx][torch.argmax(action[idx]).item()] = Q_new
    
        self.optimizer.zero_grad()
        loss = self.criterion(target, pred)
        loss.backward()
        self.optimizer.step()

================================================
File: core/replay_buffer.py
================================================
import threading
import random
from collections import deque

class ThreadSafeReplayBuffer:
    def __init__(self, maxlen=100000):
        self.memory = deque(maxlen=maxlen)
        self.lock = threading.RLock()
    
    def push(self, state, action, reward, next_state, done):
        with self.lock:
            self.memory.append((state, action, reward, next_state, done))
    
    def sample(self, batch_size):
        with self.lock:
            if len(self.memory) < batch_size:
                return list(self.memory)
            return random.sample(self.memory, batch_size)
    
    def __len__(self):
        with self.lock:
            return len(self.memory)
    
    def clear(self):
        with self.lock:
            self.memory.clear()

class PrioritizedReplayBuffer:
    def __init__(self, maxlen=100000, alpha=0.6):
        self.memory = deque(maxlen=maxlen)
        self.priorities = deque(maxlen=maxlen)
        self.alpha = alpha
        self.lock = threading.RLock()
    
    def push(self, state, action, reward, next_state, done, priority=None):
        with self.lock:
            if priority is None:
                priority = max(self.priorities) if self.priorities else 1.0
            
            self.memory.append((state, action, reward, next_state, done))
            self.priorities.append(priority)
    
    def sample(self, batch_size, beta=0.4):
        with self.lock:
            if len(self.memory) < batch_size:
                return list(self.memory), [1.0] * len(self.memory), list(range(len(self.memory)))
            
            priorities = list(self.priorities)
            probs = [p ** self.alpha for p in priorities]
            probs = [p / sum(probs) for p in probs]
            
            indices = random.choices(range(len(self.memory)), weights=probs, k=batch_size)
            samples = [self.memory[i] for i in indices]
            
            weights = [(len(self.memory) * probs[i]) ** (-beta) for i in indices]
            max_weight = max(weights)
            weights = [w / max_weight for w in weights]
            
            return samples, weights, indices
    
    def update_priorities(self, indices, priorities):
        with self.lock:
            for idx, priority in zip(indices, priorities):
                if idx < len(self.priorities):
                    self.priorities[idx] = priority
    
    def __len__(self):
        with self.lock:
            return len(self.memory)

================================================
File: game_env/base_env.py
================================================
from abc import ABC, abstractmethod
import numpy as np

class BaseForexEnv(ABC):
    def __init__(self, data, initial_balance=10000.0):
        self.data = data
        self.initial_balance = initial_balance
        self.reset()
    
    @abstractmethod
    def reset(self):
        pass
    
    @abstractmethod
    def step(self, action):
        pass
    
    @abstractmethod
    def get_state(self):
        pass
    
    @abstractmethod
    def get_current_price(self):
        pass
    
    @abstractmethod
    def get_portfolio_value(self):
        pass
    
    def render(self, mode='human'):
        pass
    
    def close(self):
        pass
    
    @property
    def action_space(self):
        return 3
    
    @property 
    def observation_space(self):
        return 4

================================================
File: game_env/forex_env.py
================================================
import numpy as np
import pandas as pd
from collections import namedtuple
import threading
from queue import Queue

Position = namedtuple('Position', 'direction entry_price entry_step')

class TemporalConstraints:
    def __init__(self):
        self.current_data_point = None
        self.data_queue = Queue(maxsize=1)
        self.synchronization_events = {
            'F1': threading.Event(),
            'F2': threading.Event(), 
            'F3': threading.Event()
        }
    
    def set_current_data(self, data_point):
        self.current_data_point = data_point
    
    def get_current_data_only(self):
        return self.current_data_point

class ForexEnv:
    def __init__(self, data, initial_balance=10000.0):
        self.data = data
        self.initial_balance = initial_balance
        self.temporal_constraints = TemporalConstraints()
        self.reset()

    def reset(self):
        self.current_step = 0
        self.balance = self.initial_balance
        self.position = None
        self.trades_history = []
        self.portfolio_value = self.initial_balance

    def get_current_price(self):
        current_data = self.temporal_constraints.get_current_data_only()
        if current_data is not None:
            return current_data['close']
        return self.data.iloc[self.current_step]['close']

    def get_portfolio_value(self):
        return self.portfolio_value

    def get_state(self):
        current_data = self.temporal_constraints.get_current_data_only()
        if current_data is None:
            current_data = self.data.iloc[self.current_step]
        
        current_price = current_data['close']
        
        if self.current_step >= 5:
            price_5_ago = self.data.iloc[self.current_step - 5]['close']
            momentum = (current_price - price_5_ago) / price_5_ago
            momentum = np.clip(momentum, -0.05, 0.05) / 0.05
        else:
            momentum = 0.0
        
        position_state = 0.0
        if self.position is not None:
            position_state = 1.0 if self.position.direction == 1 else -1.0
        
        unrealized_pnl = 0.0
        if self.position is not None:
            if self.position.direction == 1:
                unrealized_pnl = (current_price - self.position.entry_price) / self.position.entry_price
            else:
                unrealized_pnl = (self.position.entry_price - current_price) / self.position.entry_price
            unrealized_pnl = np.clip(unrealized_pnl, -0.2, 0.2) / 0.2
        
        time_factor = (self.current_step % 24) / 24.0
        
        return np.array([momentum, position_state, unrealized_pnl, time_factor], dtype=float)

    def step(self, action):
        reward = 0
        done = False
        
        current_data = self.temporal_constraints.get_current_data_only()
        if current_data is None:
            current_data = self.data.iloc[self.current_step]
        
        self.temporal_constraints.set_current_data(current_data)
        current_price = current_data['close']

        if action == 1 and self.position is None:
            self.position = Position(direction=1, entry_price=current_price, entry_step=self.current_step)
        elif action == 2 and self.position is None:
            self.position = Position(direction=-1, entry_price=current_price, entry_step=self.current_step)
        elif action == 0 and self.position is not None:
            if self.position.direction == 1:
                pnl = current_price - self.position.entry_price
            else:
                pnl = self.position.entry_price - current_price
                
            reward = 10 if pnl > 0 else -10
            
            self.balance += pnl
            self.portfolio_value = self.balance
            
            self.trades_history.append({
                'entry_price': self.position.entry_price,
                'exit_price': current_price,
                'direction': self.position.direction,
                'pnl': pnl,
                'entry_step': self.position.entry_step,
                'exit_step': self.current_step
            })
            
            self.position = None
            
        self.current_step += 1
        
        if self.current_step >= len(self.data) - 1:
            done = True
            if self.position is not None:
                if self.position.direction == 1:
                    pnl = current_price - self.position.entry_price
                else:
                    pnl = self.position.entry_price - current_price
                reward = 10 if pnl > 0 else -10
                self.balance += pnl
                self.portfolio_value = self.balance
                self.position = None
                
        return reward, done, self.get_portfolio_value()

================================================
File: game_env/gym_adapter.py
================================================


================================================
File: game_env/hierarchical_env.py
================================================


================================================
File: integration/data_feed.py
================================================
import threading
from queue import Queue, Empty
import pandas as pd
import numpy as np
import time
from pathlib import Path

class DataFeedThread(threading.Thread):
    def __init__(self, data=None, csv_path=None, data_queue=None, synchronization_events=None, timeout=5.0):
        super().__init__()
        
        if csv_path is not None:
            self.data = self._load_csv_data(csv_path)
            self._validate_csv_data()
        else:
            self.data = data
            
        self.data_queue = data_queue
        self.sync_events = synchronization_events
        self.current_index = 0
        self.timeout = timeout
        self.daemon = True
        self.stop_event = threading.Event()

    def run(self):
        while self.current_index < len(self.data) and not self.stop_event.is_set():
            new_data_point = self.data.iloc[self.current_index]
            
            if self._is_market_gap(self.current_index):
                self.current_index += 1
                continue
            
            try:
                self.data_queue.put(new_data_point, timeout=self.timeout)
            except:
                break
            
            self.sync_events['F2'].set()
            
            if not self.sync_events['F1'].wait(timeout=self.timeout):
                break
            
            self.sync_events['F1'].clear()
            self.current_index += 1
    
    def _is_market_gap(self, index):
        if index == 0:
            return False
        
        current_time = pd.to_datetime(self.data.iloc[index]['timestamp'])
        prev_time = pd.to_datetime(self.data.iloc[index-1]['timestamp'])
        time_diff = (current_time - prev_time).total_seconds() / 3600
        
        return time_diff > 2
    
    def _load_csv_data(self, csv_path):
        csv_path = Path(csv_path)
        if not csv_path.exists():
            raise FileNotFoundError(f"CSV file not found: {csv_path}")
        
        data = pd.read_csv(csv_path)
        
        if data.empty:
            raise ValueError("CSV file is empty")
        
        try:
            data['timestamp'] = pd.to_datetime(data['timestamp'])
        except:
            raise ValueError("Cannot parse timestamp column")
        
        data = data.sort_values('timestamp').reset_index(drop=True)
        
        price_cols = ['open', 'high', 'low', 'close']
        for col in price_cols:
            if col in data.columns:
                data[col] = pd.to_numeric(data[col], errors='coerce')
        
        data = data.dropna()
        
        if len(data) == 0:
            raise ValueError("No valid data after preprocessing")
        
        data = self._detect_market_gaps(data)
        data = self._normalize_features(data)
        
        return data
    
    def _detect_market_gaps(self, data):
        data = data.copy()
        data['time_diff'] = data['timestamp'].diff().dt.total_seconds() / 3600
        data['is_gap'] = data['time_diff'] > 2
        return data
    
    def _normalize_features(self, data):
        data = data.copy()
        
        price_cols = ['open', 'high', 'low', 'close']
        for col in price_cols:
            if col in data.columns:
                data[f'{col}_normalized'] = data[col] / data[col].iloc[0]
        
        if 'volume' in data.columns:
            data['volume_norm'] = (data['volume'] - data['volume'].min()) / (data['volume'].max() - data['volume'].min())
        
        return data
    
    def _validate_csv_data(self):
        required_columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume']
        missing_cols = [col for col in required_columns if col not in self.data.columns]
        if missing_cols:
            raise ValueError(f"CSV missing required columns: {missing_cols}")
        
        if self.data.empty:
            raise ValueError("CSV data is empty")
        
        if self.data['timestamp'].isnull().any():
            raise ValueError("CSV contains null timestamps")
    
    def stop(self):
        self.stop_event.set()

class TradingLogicThread(threading.Thread):
    def __init__(self, agent, env, data_queue, orders_queue, synchronization_events, timeout=5.0):
        super().__init__()
        self.agent = agent
        self.env = env
        self.data_queue = data_queue
        self.orders_queue = orders_queue
        self.sync_events = synchronization_events
        self.timeout = timeout
        self.daemon = True
        self.stop_event = threading.Event()
        self.performance_metrics = {'decision_times': []}

    def run(self):
        while not self.stop_event.is_set():
            if not self.sync_events['F2'].wait(timeout=self.timeout):
                continue
            
            try:
                current_data = self.data_queue.get(timeout=self.timeout)
            except Empty:
                continue
            
            start_time = time.time()
            
            self.env.temporal_constraints.set_current_data(current_data)
            
            state = self.agent.get_state(self.env)
            action = self.agent.get_action(state)
            
            decision_time = time.time() - start_time
            self.performance_metrics['decision_times'].append(decision_time)
            
            if any(action):
                action_index = action.index(1)
                try:
                    self.orders_queue.put({
                        'action': action_index,
                        'data': current_data,
                        'state': state,
                        'decision_time': decision_time
                    }, timeout=self.timeout)
                except:
                    pass
            
            self.sync_events['F3'].set()
            
            if not self.sync_events['F1'].wait(timeout=self.timeout):
                break
            
            self.sync_events['F2'].clear()
    
    def stop(self):
        self.stop_event.set()
    
    def get_performance_stats(self):
        if not self.performance_metrics['decision_times']:
            return {}
        
        times = self.performance_metrics['decision_times']
        return {
            'avg_decision_time': sum(times) / len(times),
            'max_decision_time': max(times),
            'decisions_count': len(times)
        }

class ExecutionThread(threading.Thread):
    def __init__(self, env, orders_queue, synchronization_events, timeout=5.0):
        super().__init__()
        self.env = env
        self.orders_queue = orders_queue
        self.sync_events = synchronization_events
        self.timeout = timeout
        self.daemon = True
        self.stop_event = threading.Event()

    def run(self):
        while not self.stop_event.is_set():
            if not self.sync_events['F3'].wait(timeout=self.timeout):
                continue
            
            while not self.orders_queue.empty():
                try:
                    order = self.orders_queue.get(timeout=0.1)
                    reward, done, portfolio_value = self.env.step(order['action'])
                    
                    if done:
                        break
                except Empty:
                    break
                except Exception as e:
                    break
            
            self.sync_events['F1'].set()
            self.sync_events['F3'].clear()
    
    def stop(self):
        self.stop_event.set()

class SequentialProcessor:
    def __init__(self, agent, env):
        self.agent = agent
        self.env = env
    
    def process_episode(self, data):
        for i, row in data.iterrows():
            self.env.temporal_constraints.set_current_data(row)
            self.env.current_step = i
            
            state = self.agent.get_state(self.env)
            action = self.agent.get_action(state)
            
            if any(action):
                action_index = action.index(1)
                reward, done, portfolio_value = self.env.step(action_index)
                
                if done:
                    break
        
        return self.env.get_portfolio_value()

class BatchCSVProcessor:
    def __init__(self, csv_path, batch_size=1000):
        self.csv_path = csv_path
        self.batch_size = batch_size
        self.data = self._load_csv_data(csv_path)
    
    def _load_csv_data(self, csv_path):
        csv_path = Path(csv_path)
        if not csv_path.exists():
            raise FileNotFoundError(f"CSV file not found: {csv_path}")
        
        data = pd.read_csv(csv_path)
        
        if data.empty:
            raise ValueError("CSV file is empty")
        
        try:
            data['timestamp'] = pd.to_datetime(data['timestamp'])
        except:
            raise ValueError("Cannot parse timestamp column")
        
        data = data.sort_values('timestamp').reset_index(drop=True)
        
        price_cols = ['open', 'high', 'low', 'close']
        for col in price_cols:
            if col in data.columns:
                data[col] = pd.to_numeric(data[col], errors='coerce')
        
        data = data.dropna()
        
        if len(data) == 0:
            raise ValueError("No valid data after preprocessing")
        
        data = self._detect_market_gaps(data)
        data = self._normalize_features(data)
        
        return data
    
    def _detect_market_gaps(self, data):
        data = data.copy()
        data['time_diff'] = data['timestamp'].diff().dt.total_seconds() / 3600
        data['is_gap'] = data['time_diff'] > 2
        return data
    
    def _normalize_features(self, data):
        data = data.copy()
        
        price_cols = ['open', 'high', 'low', 'close']
        for col in price_cols:
            if col in data.columns:
                data[f'{col}_normalized'] = data[col] / data[col].iloc[0]
        
        if 'volume' in data.columns:
            data['volume_norm'] = (data['volume'] - data['volume'].min()) / (data['volume'].max() - data['volume'].min())
        
        return data
        
    def get_batches(self):
        for i in range(0, len(self.data), self.batch_size):
            yield self.data.iloc[i:i+self.batch_size]
    
    def process_all_batches(self, agent, env):
        total_episodes = 0
        for batch in self.get_batches():
            processor = SequentialProcessor(agent, env)
            processor.process_episode(batch)
            total_episodes += 1
            env.reset()
        
        return total_episodes

================================================
File: integration/dwx_connector.py
================================================


================================================
File: integration/signal_translator.py
================================================


================================================
File: tests/test_agent.py
================================================
import unittest
import numpy as np
import torch
from core.agent import Agent
from env.forex_env import ForexEnv
import pandas as pd

class TestAgent(unittest.TestCase):
    def setUp(self):
        self.agent = Agent()
        
        sample_data = pd.DataFrame({
            'timestamp': pd.date_range('2023-01-01', periods=100, freq='H'),
            'open': np.random.uniform(1.2000, 1.2100, 100),
            'high': np.random.uniform(1.2000, 1.2100, 100),
            'low': np.random.uniform(1.2000, 1.2100, 100),
            'close': np.random.uniform(1.2000, 1.2100, 100),
            'volume': np.random.randint(1000, 5000, 100)
        })
        self.env = ForexEnv(sample_data)
    
    def test_agent_initialization(self):
        self.assertEqual(self.agent.n_games, 0)
        self.assertEqual(self.agent.epsilon, 0)
        self.assertEqual(self.agent.gamma, 0.9)
        self.assertIsNotNone(self.agent.model)
        self.assertIsNotNone(self.agent.trainer)
        self.assertEqual(len(self.agent.memory), 0)
    
    def test_get_state(self):
        state = self.agent.get_state(self.env)
        self.assertEqual(len(state), 4)
        self.assertTrue(isinstance(state, np.ndarray))
        self.assertTrue(all(isinstance(x, (int, float, np.number)) for x in state))
    
    def test_get_action_random(self):
        self.agent.epsilon = 100
        state = np.array([0.1, 0.2, 0.3, 0.4])
        action = self.agent.get_action(state)
        
        self.assertEqual(len(action), 3)
        self.assertEqual(sum(action), 1)
        self.assertTrue(any(action))
    
    def test_get_action_model(self):
        self.agent.epsilon = 0
        state = np.array([0.1, 0.2, 0.3, 0.4])
        action = self.agent.get_action(state)
        
        self.assertEqual(len(action), 3)
        self.assertEqual(sum(action), 1)
        self.assertTrue(any(action))
    
    def test_remember(self):
        state = np.array([0.1, 0.2, 0.3, 0.4])
        action = [1, 0, 0]
        reward = 10
        next_state = np.array([0.2, 0.3, 0.4, 0.5])
        done = False
        
        initial_memory_size = len(self.agent.memory)
        self.agent.remember(state, action, reward, next_state, done)
        
        self.assertEqual(len(self.agent.memory), initial_memory_size + 1)
        
        stored_experience = self.agent.memory[-1]
        self.assertTrue(np.array_equal(stored_experience[0], state))
        self.assertEqual(stored_experience[1], action)
        self.assertEqual(stored_experience[2], reward)
        self.assertTrue(np.array_equal(stored_experience[3], next_state))
        self.assertEqual(stored_experience[4], done)
    
    def test_train_short_memory(self):
        state = np.array([0.1, 0.2, 0.3, 0.4])
        action = [1, 0, 0]
        reward = 10
        next_state = np.array([0.2, 0.3, 0.4, 0.5])
        done = False
        
        try:
            self.agent.train_short_memory(state, action, reward, next_state, done)
        except Exception as e:
            self.fail(f"train_short_memory raised an exception: {e}")
    
    def test_train_long_memory_empty(self):
        try:
            self.agent.train_long_memory()
        except Exception as e:
            self.fail(f"train_long_memory with empty memory raised an exception: {e}")
    
    def test_train_long_memory_with_data(self):
        for i in range(10):
            state = np.random.rand(4)
            action = [0, 0, 0]
            action[np.random.randint(3)] = 1
            reward = np.random.randint(-10, 11)
            next_state = np.random.rand(4)
            done = np.random.choice([True, False])
            
            self.agent.remember(state, action, reward, next_state, done)
        
        try:
            self.agent.train_long_memory()
        except Exception as e:
            self.fail(f"train_long_memory with data raised an exception: {e}")
    
    def test_model_consistency(self):
        state = np.array([0.1, 0.2, 0.3, 0.4])
        
        with torch.no_grad():
            self.agent.epsilon = 0
            action1 = self.agent.get_action(state)
            action2 = self.agent.get_action(state)
            
            self.assertEqual(action1, action2)
    
    def test_epsilon_decay_behavior(self):
        initial_epsilon = self.agent.epsilon
        self.agent.n_games = 50
        
        state = np.array([0.1, 0.2, 0.3, 0.4])
        action = self.agent.get_action(state)
        
        expected_epsilon = 80 - 50
        self.assertEqual(expected_epsilon, 30)
    
    def test_memory_maxlen(self):
        from core.agent import MAX_MEMORY
        
        for i in range(MAX_MEMORY + 100):
            state = np.random.rand(4)
            action = [0, 0, 0]
            action[np.random.randint(3)] = 1
            reward = np.random.randint(-10, 11)
            next_state = np.random.rand(4)
            done = False
            
            self.agent.remember(state, action, reward, next_state, done)
        
        self.assertEqual(len(self.agent.memory), MAX_MEMORY)

if __name__ == '__main__':
    unittest.main()

================================================
File: tests/test_env.py
================================================
import unittest
import numpy as np
import pandas as pd
from env.forex_env import ForexEnv

class TestForexEnv(unittest.TestCase):
    def setUp(self):
        self.sample_data = pd.DataFrame({
            'timestamp': pd.date_range('2023-01-01', periods=100, freq='H'),
            'open': np.linspace(1.2000, 1.2100, 100),
            'high': np.linspace(1.2010, 1.2110, 100),
            'low': np.linspace(1.1990, 1.2090, 100),
            'close': np.linspace(1.2005, 1.2105, 100),
            'volume': np.random.randint(1000, 5000, 100)
        })
        self.env = ForexEnv(self.sample_data)
    
    def test_env_initialization(self):
        self.assertEqual(self.env.current_step, 0)
        self.assertEqual(self.env.balance, 10000.0)
        self.assertIsNone(self.env.position)
        self.assertEqual(len(self.env.trades_history), 0)
        self.assertEqual(self.env.portfolio_value, 10000.0)
    
    def test_reset(self):
        self.env.current_step = 50
        self.env.balance = 9000.0
        self.env.portfolio_value = 9000.0
        self.env.trades_history = [{'test': 'data'}]
        
        self.env.reset()
        
        self.assertEqual(self.env.current_step, 0)
        self.assertEqual(self.env.balance, 10000.0)
        self.assertIsNone(self.env.position)
        self.assertEqual(len(self.env.trades_history), 0)
        self.assertEqual(self.env.portfolio_value, 10000.0)
    
    def test_get_current_price(self):
        price = self.env.get_current_price()
        expected_price = self.sample_data.iloc[0]['close']
        self.assertEqual(price, expected_price)
        
        self.env.current_step = 10
        price = self.env.get_current_price()
        expected_price = self.sample_data.iloc[10]['close']
        self.assertEqual(price, expected_price)
    
    def test_get_state_shape(self):
        state = self.env.get_state()
        self.assertEqual(len(state), 4)
        self.assertTrue(isinstance(state, np.ndarray))
    
    def test_get_state_no_position(self):
        state = self.env.get_state()
        
        self.assertTrue(-1 <= state[0] <= 1)
        self.assertEqual(state[1], 0.0)
        self.assertEqual(state[2], 0.0)
        self.assertTrue(0 <= state[3] <= 1)
    
    def test_step_action_1_long_entry(self):
        initial_balance = self.env.balance
        reward, done, portfolio_value = self.env.step(1)
        
        self.assertIsNotNone(self.env.position)
        self.assertEqual(self.env.position.direction, 1)
        self.assertEqual(reward, 0)
        self.assertFalse(done)
        self.assertEqual(self.env.balance, initial_balance)
    
    def test_step_action_2_short_entry(self):
        initial_balance = self.env.balance
        reward, done, portfolio_value = self.env.step(2)
        
        self.assertIsNotNone(self.env.position)
        self.assertEqual(self.env.position.direction, -1)
        self.assertEqual(reward, 0)
        self.assertFalse(done)
        self.assertEqual(self.env.balance, initial_balance)
    
    def test_step_action_0_no_position(self):
        initial_balance = self.env.balance
        reward, done, portfolio_value = self.env.step(0)
        
        self.assertIsNone(self.env.position)
        self.assertEqual(reward, 0)
        self.assertFalse(done)
        self.assertEqual(self.env.balance, initial_balance)
    
    def test_long_position_profitable_close(self):
        self.env.step(1)
        
        self.env.current_step = 10
        initial_balance = self.env.balance
        entry_price = self.env.position.entry_price
        current_price = self.env.get_current_price()
        
        self.assertGreater(current_price, entry_price)
        
        reward, done, portfolio_value = self.env.step(0)
        
        self.assertIsNone(self.env.position)
        self.assertEqual(reward, 10)
        self.assertGreater(self.env.balance, initial_balance)
        self.assertEqual(len(self.env.trades_history), 1)
    
    def test_short_position_profitable_close(self):
        self.env.current_step = 50
        self.env.step(2)
        
        entry_price = self.env.position.entry_price
        
        self.env.current_step = 10
        current_price = self.env.get_current_price()
        
        self.assertLess(current_price, entry_price)
        
        initial_balance = self.env.balance
        reward, done, portfolio_value = self.env.step(0)
        
        self.assertIsNone(self.env.position)
        self.assertEqual(reward, 10)
        self.assertGreater(self.env.balance, initial_balance)
        self.assertEqual(len(self.env.trades_history), 1)
    
    def test_long_position_losing_close(self):
        self.env.current_step = 50
        self.env.step(1)
        
        entry_price = self.env.position.entry_price
        
        self.env.current_step = 10
        current_price = self.env.get_current_price()
        
        self.assertLess(current_price, entry_price)
        
        initial_balance = self.env.balance
        reward, done, portfolio_value = self.env.step(0)
        
        self.assertIsNone(self.env.position)
        self.assertEqual(reward, -10)
        self.assertLess(self.env.balance, initial_balance)
        self.assertEqual(len(self.env.trades_history), 1)
    
    def test_episode_termination(self):
        self.env.current_step = len(self.sample_data) - 2
        reward, done, portfolio_value = self.env.step(1)
        
        self.assertTrue(done)
    
    def test_position_state_with_long(self):
        self.env.step(1)
        state = self.env.get_state()
        
        self.assertEqual(state[1], 1.0)
    
    def test_position_state_with_short(self):
        self.env.step(2)
        state = self.env.get_state()
        
        self.assertEqual(state[1], -1.0)
    
    def test_unrealized_pnl_calculation(self):
        self.env.step(1)
        
        self.env.current_step = 10
        state = self.env.get_state()
        
        self.assertNotEqual(state[2], 0.0)
        self.assertTrue(-1 <= state[2] <= 1)
    
    def test_trades_history_structure(self):
        self.env.step(1)
        self.env.current_step = 10
        self.env.step(0)
        
        self.assertEqual(len(self.env.trades_history), 1)
        trade = self.env.trades_history[0]
        
        required_keys = ['entry_price', 'exit_price', 'direction', 'pnl', 'entry_step', 'exit_step']
        for key in required_keys:
            self.assertIn(key, trade)
    
    def test_multiple_positions_not_allowed(self):
        self.env.step(1)
        
        initial_position = self.env.position
        self.env.step(2)
        
        self.assertEqual(self.env.position, initial_position)
    
    def test_temporal_constraints_integration(self):
        test_data_point = {
            'timestamp': pd.Timestamp('2023-01-01 12:00:00'),
            'open': 1.2050,
            'high': 1.2060,
            'low': 1.2040,
            'close': 1.2055,
            'volume': 2000
        }
        
        self.env.temporal_constraints.set_current_data(test_data_point)
        
        price = self.env.get_current_price()
        self.assertEqual(price, 1.2055)

if __name__ == '__main__':
    unittest.main()

================================================
File: tests/test_trainer.py
================================================


================================================
File: training/evaluator.py
================================================
import numpy as np
from utils.metrics import TradingMetrics

class TradingEvaluator:
    def __init__(self):
        self.metrics = TradingMetrics()
        
    def evaluate_episode(self, env, final_balance, initial_balance=10000):
        self.metrics.add_balance(final_balance)
        
        for trade in env.trades_history:
            self.metrics.add_trade(
                trade['entry_price'],
                trade['exit_price'], 
                trade['direction'],
                trade['entry_step'],
                trade['exit_step']
            )
        
        episode_return = (final_balance - initial_balance) / initial_balance
        self.metrics.add_episode_reward(episode_return)
        
        return {
            'final_balance': final_balance,
            'episode_return': episode_return,
            'total_trades': len(env.trades_history),
            'current_metrics': self.metrics.get_summary()
        }
    
    def evaluate_agent_performance(self, agent, env, num_episodes=100):
        episode_scores = []
        episode_balances = []
        
        for episode in range(num_episodes):
            env.reset()
            
            while True:
                state = agent.get_state(env)
                action = agent.get_action(state)
                
                if any(action):
                    action_index = action.index(1)
                    reward, done, portfolio_value = env.step(action_index)
                    
                    if done:
                        episode_scores.append(portfolio_value)
                        episode_balances.append(portfolio_value)
                        break
                else:
                    break
        
        return {
            'avg_balance': np.mean(episode_balances),
            'std_balance': np.std(episode_balances),
            'min_balance': np.min(episode_balances),
            'max_balance': np.max(episode_balances),
            'success_rate': sum(1 for b in episode_balances if b > 10000) / len(episode_balances),
            'all_balances': episode_balances
        }
    
    def get_performance_summary(self):
        return self.metrics.get_summary()
    
    def reset_metrics(self):
        self.metrics.reset()

================================================
File: training/trainer.py
================================================
from core.agent import Agent
from env.forex_env import ForexEnv
from integration.data_feed import SequentialProcessor, BatchCSVProcessor, DataFeedThread

class OfflineTrainer:
    def __init__(self, csv_path, max_episodes=1000):
        self.csv_path = csv_path
        self.max_episodes = max_episodes
        self.agent = Agent()
        
        data_feed = DataFeedThread(csv_path=csv_path)
        self.data = data_feed.data
        self.env = ForexEnv(self.data)
        
    def train_single_episode(self):
        sequential_processor = SequentialProcessor(self.agent, self.env)
        final_balance = sequential_processor.process_episode(self.data)
        
        self.agent.n_games += 1
        self.agent.train_long_memory()
        
        return final_balance
    
    def train_batch_episodes(self, batch_size=1000):
        batch_processor = BatchCSVProcessor(self.csv_path, batch_size)
        total_episodes = batch_processor.process_all_batches(self.agent, self.env)
        
        self.agent.train_long_memory()
        return total_episodes
    
    def train(self, use_batches=False, batch_size=1000):
        best_score = 0
        scores = []
        
        for episode in range(self.max_episodes):
            if use_batches:
                total_episodes = self.train_batch_episodes(batch_size)
                final_balance = self.env.get_portfolio_value()
            else:
                final_balance = self.train_single_episode()
            
            scores.append(final_balance)
            
            if final_balance > best_score:
                best_score = final_balance
                self.agent.model.save(f'best_model_episode_{episode}.pth')
            
            self.env.reset()
            
            if episode % 10 == 0:
                avg_score = sum(scores[-10:]) / min(10, len(scores))
                print(f'Episode {episode}, Balance: ${final_balance:.2f}, Avg: ${avg_score:.2f}, Best: ${best_score:.2f}')
        
        return scores

================================================
File: utils/logger.py
================================================
import os
import json
import pickle
from datetime import datetime
from pathlib import Path

class TradingLogger:
    def __init__(self, log_dir="./logs"):
        self.log_dir = Path(log_dir)
        self.log_dir.mkdir(exist_ok=True)
        
        self.session_id = datetime.now().strftime("%Y%m%d_%H%M%S")
        self.episode_log = []
        self.model_dir = self.log_dir / "models"
        self.model_dir.mkdir(exist_ok=True)
        
    def log_episode(self, episode, final_balance, trades_count, epsilon, loss=None):
        entry = {
            'episode': episode,
            'timestamp': datetime.now().isoformat(),
            'final_balance': final_balance,
            'trades_count': trades_count,
            'epsilon': epsilon,
            'loss': loss
        }
        self.episode_log.append(entry)
        
        if episode % 10 == 0:
            self._save_episode_log()
    
    def log_trade(self, trade_data):
        trade_log_path = self.log_dir / f"trades_{self.session_id}.json"
        
        with open(trade_log_path, 'a') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                **trade_data
            }, f)
            f.write('\n')
    
    def save_checkpoint(self, agent, episode, final_balance):
        checkpoint = {
            'episode': episode,
            'final_balance': final_balance,
            'model_state': agent.model.state_dict(),
            'agent_params': {
                'n_games': agent.n_games,
                'epsilon': agent.epsilon,
                'gamma': agent.gamma
            },
            'timestamp': datetime.now().isoformat()
        }
        
        checkpoint_path = self.model_dir / f"checkpoint_episode_{episode}.pkl"
        with open(checkpoint_path, 'wb') as f:
            pickle.dump(checkpoint, f)
        
        latest_path = self.model_dir / "latest_checkpoint.pkl"
        with open(latest_path, 'wb') as f:
            pickle.dump(checkpoint, f)
    
    def load_checkpoint(self, agent, checkpoint_path=None):
        if checkpoint_path is None:
            checkpoint_path = self.model_dir / "latest_checkpoint.pkl"
        
        if not checkpoint_path.exists():
            return False
        
        with open(checkpoint_path, 'rb') as f:
            checkpoint = pickle.load(f)
        
        agent.model.load_state_dict(checkpoint['model_state'])
        agent.n_games = checkpoint['agent_params']['n_games']
        agent.epsilon = checkpoint['agent_params']['epsilon']
        agent.gamma = checkpoint['agent_params']['gamma']
        
        return True
    
    def log_performance_metrics(self, metrics):
        metrics_path = self.log_dir / f"metrics_{self.session_id}.json"
        
        with open(metrics_path, 'w') as f:
            json.dump({
                'timestamp': datetime.now().isoformat(),
                'session_id': self.session_id,
                **metrics
            }, f, indent=2)
    
    def _save_episode_log(self):
        episode_log_path = self.log_dir / f"episodes_{self.session_id}.json"
        
        with open(episode_log_path, 'w') as f:
            json.dump(self.episode_log, f, indent=2)
    
    def get_session_summary(self):
        if not self.episode_log:
            return {}
        
        balances = [entry['final_balance'] for entry in self.episode_log]
        
        return {
            'session_id': self.session_id,
            'total_episodes': len(self.episode_log),
            'avg_balance': sum(balances) / len(balances),
            'max_balance': max(balances),
            'min_balance': min(balances),
            'final_balance': balances[-1] if balances else 0
        }

================================================
File: utils/metrics.py
================================================
import numpy as np

class TradingMetrics:
    def __init__(self):
        self.reset()
    
    def reset(self):
        self.trades = []
        self.balance_history = []
        self.episode_rewards = []
    
    def add_trade(self, entry_price, exit_price, direction, entry_step, exit_step):
        pnl = (exit_price - entry_price) if direction == 1 else (entry_price - exit_price)
        
        trade = {
            'entry_price': entry_price,
            'exit_price': exit_price,
            'direction': direction,
            'pnl': pnl,
            'entry_step': entry_step,
            'exit_step': exit_step,
            'duration': exit_step - entry_step
        }
        self.trades.append(trade)
    
    def add_balance(self, balance):
        self.balance_history.append(balance)
    
    def add_episode_reward(self, reward):
        self.episode_rewards.append(reward)
    
    def get_win_rate(self):
        if not self.trades:
            return 0.0
        
        winning_trades = sum(1 for trade in self.trades if trade['pnl'] > 0)
        return winning_trades / len(self.trades)
    
    def get_avg_profit(self):
        if not self.trades:
            return 0.0
        
        profits = [trade['pnl'] for trade in self.trades if trade['pnl'] > 0]
        return np.mean(profits) if profits else 0.0
    
    def get_avg_loss(self):
        if not self.trades:
            return 0.0
        
        losses = [trade['pnl'] for trade in self.trades if trade['pnl'] < 0]
        return np.mean(losses) if losses else 0.0
    
    def get_max_drawdown(self):
        if len(self.balance_history) < 2:
            return 0.0
        
        peak = self.balance_history[0]
        max_dd = 0.0
        
        for balance in self.balance_history:
            if balance > peak:
                peak = balance
            
            drawdown = (peak - balance) / peak if peak > 0 else 0.0
            max_dd = max(max_dd, drawdown)
        
        return max_dd
    
    def get_sharpe_ratio(self):
        if len(self.episode_rewards) < 2:
            return 0.0
        
        returns = np.array(self.episode_rewards)
        return np.mean(returns) / np.std(returns) if np.std(returns) > 0 else 0.0
    
    def get_total_pnl(self):
        return sum(trade['pnl'] for trade in self.trades)
    
    def get_summary(self):
        return {
            'total_trades': len(self.trades),
            'win_rate': self.get_win_rate(),
            'avg_profit': self.get_avg_profit(),
            'avg_loss': self.get_avg_loss(),
            'total_pnl': self.get_total_pnl(),
            'max_drawdown': self.get_max_drawdown(),
            'sharpe_ratio': self.get_sharpe_ratio(),
            'final_balance': self.balance_history[-1] if self.balance_history else 0.0
        }

================================================
File: utils/plots.py
================================================
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from pathlib import Path

class TradingPlots:
    def __init__(self, save_dir="./plots"):
        self.save_dir = Path(save_dir)
        self.save_dir.mkdir(exist_ok=True)
        
    def plot_equity_curve(self, balance_history, title="Equity Curve", save_name=None):
        plt.figure(figsize=(12, 6))
        plt.plot(balance_history, linewidth=2)
        plt.title(title)
        plt.xlabel("Episode")
        plt.ylabel("Balance ($)")
        plt.grid(True, alpha=0.3)
        
        if save_name:
            plt.savefig(self.save_dir / f"{save_name}.png", dpi=300, bbox_inches='tight')
        
        plt.close()
    
    def plot_drawdown(self, balance_history, title="Drawdown Analysis", save_name=None):
        balance_series = pd.Series(balance_history)
        peak = balance_series.expanding().max()
        drawdown = (balance_series - peak) / peak * 100
        
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
        
        ax1.plot(balance_history, linewidth=2, label='Balance')
        ax1.plot(peak, linewidth=1, alpha=0.7, label='Peak')
        ax1.set_title("Balance vs Peak")
        ax1.set_ylabel("Balance ($)")
        ax1.legend()
        ax1.grid(True, alpha=0.3)
        
        ax2.fill_between(range(len(drawdown)), drawdown, 0, alpha=0.3, color='red')
        ax2.plot(drawdown, color='red', linewidth=1)
        ax2.set_title("Drawdown %")
        ax2.set_xlabel("Episode")
        ax2.set_ylabel("Drawdown (%)")
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_name:
            plt.savefig(self.save_dir / f"{save_name}.png", dpi=300, bbox_inches='tight')
        
        plt.close()
    
    def plot_trade_analysis(self, trades, title="Trade Analysis", save_name=None):
        if not trades:
            return
        
        profits = [t['pnl'] for t in trades if t['pnl'] > 0]
        losses = [t['pnl'] for t in trades if t['pnl'] < 0]
        
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        ax1.hist([t['pnl'] for t in trades], bins=30, alpha=0.7, edgecolor='black')
        ax1.set_title("PnL Distribution")
        ax1.set_xlabel("PnL")
        ax1.set_ylabel("Frequency")
        ax1.grid(True, alpha=0.3)
        
        if profits and losses:
            ax2.hist([profits, losses], bins=20, alpha=0.7, 
                    label=['Profits', 'Losses'], color=['green', 'red'])
            ax2.set_title("Profit vs Loss Distribution")
            ax2.set_xlabel("PnL")
            ax2.set_ylabel("Frequency")
            ax2.legend()
            ax2.grid(True, alpha=0.3)
        
        durations = [t['exit_step'] - t['entry_step'] for t in trades]
        ax3.hist(durations, bins=20, alpha=0.7, edgecolor='black')
        ax3.set_title("Trade Duration Distribution")
        ax3.set_xlabel("Duration (steps)")
        ax3.set_ylabel("Frequency")
        ax3.grid(True, alpha=0.3)
        
        cumulative_pnl = np.cumsum([t['pnl'] for t in trades])
        ax4.plot(cumulative_pnl, linewidth=2)
        ax4.set_title("Cumulative PnL")
        ax4.set_xlabel("Trade Number")
        ax4.set_ylabel("Cumulative PnL")
        ax4.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_name:
            plt.savefig(self.save_dir / f"{save_name}.png", dpi=300, bbox_inches='tight')
        
        plt.close()
    
    def plot_training_progress(self, episode_data, title="Training Progress", save_name=None):
        episodes = [d['episode'] for d in episode_data]
        balances = [d['final_balance'] for d in episode_data]
        epsilons = [d['epsilon'] for d in episode_data]
        
        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))
        
        ax1.plot(episodes, balances, linewidth=2, color='blue')
        ax1.set_title("Training Progress - Balance")
        ax1.set_ylabel("Final Balance ($)")
        ax1.grid(True, alpha=0.3)
        
        window = min(50, len(balances) // 10) if len(balances) > 10 else 1
        if window > 1:
            moving_avg = pd.Series(balances).rolling(window=window).mean()
            ax1.plot(episodes, moving_avg, linewidth=2, color='red', alpha=0.7, 
                    label=f'MA({window})')
            ax1.legend()
        
        ax2.plot(episodes, epsilons, linewidth=2, color='orange')
        ax2.set_title("Epsilon Decay")
        ax2.set_xlabel("Episode")
        ax2.set_ylabel("Epsilon")
        ax2.grid(True, alpha=0.3)
        
        plt.tight_layout()
        
        if save_name:
            plt.savefig(self.save_dir / f"{save_name}.png", dpi=300, bbox_inches='tight')
        
        plt.close()
    
    def plot_action_distribution(self, actions, title="Action Distribution", save_name=None):
        action_names = ['Close', 'Long', 'Short']
        action_counts = [actions.count(i) for i in range(3)]
        
        plt.figure(figsize=(8, 6))
        plt.pie(action_counts, labels=action_names, autopct='%1.1f%%', startangle=90)
        plt.title(title)
        
        if save_name:
            plt.savefig(self.save_dir / f"{save_name}.png", dpi=300, bbox_inches='tight')
        
        plt.close()

================================================
File: utils/risk.py
================================================


