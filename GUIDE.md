# ðŸ“¦ Refactored Project Structure

```

v4/
â”œâ”€â”€ core/
â”‚   â”œâ”€â”€ agent.py               # Base RL agent logic (epsilon, memory, training)
â”‚   â”œâ”€â”€ model.py               # DQN architecture: Linear\_QNet, Dueling, etc.
â”‚   â”œâ”€â”€ config.py              # Central config registry
â”‚   â””â”€â”€ replay\_buffer.py       # Separate prioritized/standard buffer
â”‚
â”œâ”€â”€ env/
â”‚   â”œâ”€â”€ base\_env.py            # Base class for all ForexEnv variants
â”‚   â”œâ”€â”€ forex\_env.py           # Basic H1 environment
â”‚   â”œâ”€â”€ hierarchical\_env.py    # Multi-timeframe environment (D1/H1 logic)
â”‚   â””â”€â”€ gym\_adapter.py         # Gym wrapper
â”‚
â”œâ”€â”€ integration/
â”‚   â”œâ”€â”€ dwx\_connector.py       # Handles live trading via DWX
â”‚   â”œâ”€â”€ data\_feed.py           # Abstracts loading/syncing price data
â”‚   â””â”€â”€ signal\_translator.py   # Converts RL action to MT4-compatible signal
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ trainer.py             # Offline training loop
â”‚   â”œâ”€â”€ live\_trainer.py        # Live trading loop
â”‚   â”œâ”€â”€ grid\_search.py         # Grid search logic
â”‚   â”œâ”€â”€ config\_generator.py    # Generates training configs
â”‚   â””â”€â”€ evaluator.py           # Evaluation and stats logging
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ logger.py              # Logging, checkpoints
â”‚   â”œâ”€â”€ metrics.py             # Reward tracking, win-rate, drawdown, etc.
â”‚   â”œâ”€â”€ risk.py                # Risk rules and position sizing logic
â”‚   â””â”€â”€ plots.py               # Visualization
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test\_env.py
â”‚   â”œâ”€â”€ test\_agent.py
â”‚   â””â”€â”€ test\_trainer.py
â”‚
â”œâ”€â”€ notebooks/                 # For experiments, not core code
â”‚   â””â”€â”€ exploration.ipynb
â”‚
â”œâ”€â”€ run.py                     # CLI to launch training, evaluation, or live
â””â”€â”€ README.md

````

---

# ðŸ“ˆ Program Flow

```mermaid
flowchart TD
    Start([Start]) --> CLI{run.py CLI?}
    CLI -->|train| Train[Trainer]
    CLI -->|live| Live[LiveTrader]
    CLI -->|grid| Grid[GridSearch]

    Train --> LoadData --> InitEnv
    InitEnv --> InitAgent --> TrainLoop
    TrainLoop --> SaveModel --> End([Done])

    Live --> LoadModel --> LiveEnv --> TickLoop
    TickLoop --> Action --> Signal --> DWX --> TickLoop

    Grid --> GenConfigs --> ParallelTrain --> AnalyzeResults --> End
````

---

# ðŸ§  Pseudocode Overview

---

### `core/agent.py`

```python
class DQNAgent:
    def __init__(self, config): ...
    def act(state): ...
    def train_step(batch): ...
    def remember(...): ...
    def update_epsilon(): ...
```

---

### `core/model.py`

```python
class DQN(nn.Module):
    def __init__(input_size, output_size, config): ...
    def forward(x): ...
def create_model(config): ...
```

---

### `env/base_env.py`

```python
class BaseForexEnv:
    def reset(): ...
    def step(action): ...
    def get_state(): ...
```

---

### `env/hierarchical_env.py`

```python
class HierarchicalForexEnv(BaseForexEnv):
    def get_state(): ...
    def get_bias_from_d1(): ...
```

---

### `integration/dwx_connector.py`

```python
class DWXConnector:
    def send_signal(action): ...
    def update_risk(): ...
```

---

### `training/trainer.py`

```python
def train(agent, env, config):
    for episode in range(config.episodes):
        state = env.reset()
        while not done:
            action = agent.act(state)
            next_state, reward, done = env.step(action)
            agent.remember(...)
        agent.replay()
```

---

### `training/grid_search.py`

```python
def run_search(configs):
    for config in configs:
        agent = DQNAgent(config)
        result = train(agent, env, config)
    analyze_results()
```

---

### `run.py`

```python
if args.mode == 'train':
    config = load_config(args.config)
    train(agent, env, config)
elif args.mode == 'live':
    run_live_trading(agent, env)
elif args.mode == 'grid':
    run_search(configs)
```